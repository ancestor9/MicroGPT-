{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ancestor9/MicroGPT-/blob/main/Basic_Understanding_microgpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "대형언어모델(LLM)의 복잡한 구조를 요약, 단 200줄의 순수 파이썬 코드로 GPT의 핵심 원리를 구현한 프로젝트가 공개됐다.\n",
        "\n",
        "안드레이 카르파시가 선보인 ‘마이크로GPT(MicroGPT)’는 외부 라이브러리 없이도 GPT의 학습과 추론 과정을 모두 수행하는 단일 파일 프로그램이다.\n",
        "\n",
        "거대한 인프라나 복잡한 프레임워크 없이, LLM이 작동하는 알고리즘의 본질만을 가장 단순한 형태로 담아냈다는 평가를 받고 있다.\n",
        "\n",
        "카르파시 유레카랩스 CEO는 12일(현지시간) 블로그를 통해 ‘마이크로GPT’를 소개하며, 이를 하나의 “예술 작품(art project)”이라고 표현했다.\n",
        "\n",
        "그는 GPT를 구성하는 모든 핵심 요소를 단 하나의 파일에 담았다고 설명하며, “이보다 더 단순하게 줄일 수는 없다”라고 강조했다.\n",
        "\n",
        "마이크로GPT는 단 하나의 파이썬 파일로 구성된다. 이 파일 안에는\n",
        "- ▲데이터셋 처리\n",
        "- ▲토크나이저\n",
        "- ▲자동 미분(autograd) 엔진\n",
        "- ▲GPT-2와 유사한 신경망 구조\n",
        "- ▲아담(Adam) 옵티마이저\n",
        "- ▲학습 루프\n",
        "- ▲추론 루프   \n",
        "\n",
        "까지, GPT가 작동하는 전 과정이 모두 담겨 있다.\n",
        "\n",
        "### [원문기사](https://www.aitimes.com/news/articleView.html?idxno=206917)"
      ],
      "metadata": {
        "id": "0HItlJ9_s_fa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 원래 코드를 7개 모듈로 분해하여 학습을 목적으로 각 모듈의 핵심을 짧게 요약하면:\n",
        "\n",
        "1. 데이터셋 처리 — 이름 목록을 불러와 셔플. 문서 단위 학습의 기본 구조.\n",
        "2. 토크나이저 — 문자 ↔ 정수 변환. BOS 특수 토큰으로 시작/끝 표시.\n",
        "3. Autograd 엔진 — PyTorch 없이 스칼라 수준에서 역전파 구현. 계산 그래프 + 체인 룰의 정수.\n",
        "4. GPT 신경망 — 임베딩 → RMSNorm → Multi-Head Attention → MLP → 잔차 연결의 반복. GPT-2 아키텍처의 축소판.\n",
        "5. Adam 옵티마이저 — 1차/2차 모멘텀으로 파라미터별 적응적 학습률을 계산.\n",
        "6. 학습 루프 — 순전파 → 크로스엔트로피 손실 → 역전파 → Adam 업데이트의 반복.\n",
        "7. 추론 루프 — BOS를 시작으로 온도(temperature)에 따라 확률적으로 다음 토큰을 샘플링.\n",
        "\n",
        ">> 특히 모듈 3(Autograd) 이 이 코드의 교육적 핵심으로 PyTorch가 내부적으로 하는 일을 50줄로 직접 구현한 것이라, 이 부분을 확실히 이해하면 딥러닝의 역전파 원리 전체가 명확해집니다."
      ],
      "metadata": {
        "id": "3Ype3pu1vopG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모듈 1 ▲ 데이터셋 처리"
      ],
      "metadata": {
        "id": "wjDd-5sGvo_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os       # os.path.exists\n",
        "import math     # math.log, math.exp\n",
        "import random   # random.seed, random.choices, random.gauss, random.shuffle\n",
        "\n",
        "if not os.path.exists('input.txt'):\n",
        "    import urllib.request\n",
        "    names_url = 'https://raw.githubusercontent.com/karpathy/makemore/988aa59/names.txt'\n",
        "    urllib.request.urlretrieve(names_url, 'input.txt')\n",
        "\n",
        "docs = [line.strip() for line in open('input.txt') if line.strip()]\n",
        "random.shuffle(docs)"
      ],
      "metadata": {
        "id": "LAjcSfG_wWR1"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(docs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NhiXcD8wvJm",
        "outputId": "c248aec9-94e8-46f6-992d-56753919ffb7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "' / '.join(docs[:15])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "rovUISDBw0Oi",
        "outputId": "51b24fd5-ed1d-40cc-faaf-c990526a15c6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'kedan / dalayza / terrell / rosario / lijah / keilynn / nayah / aveen / khamiyah / chicago / greer / maziyah / janai / jerico / jahaziah'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "설명\n",
        "\n",
        "- input.txt가 없으면 인터넷에서 이름 목록(약 32,000개)을 자동 다운로드합니다.\n",
        "- 각 줄을 하나의 문서(document) 로 취급합니다. 여기서 문서 = 사람 이름 하나.\n",
        "- random.shuffle로 순서를 섞어 학습 편향을 방지합니다.\n",
        "- 핵심 개념: 언어 모델은 문서 단위로 학습합니다. 문서가 짧은 이름이든 긴 소설이든 원리는 동일합니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "Gx1AnzLJwh5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 메모리 절약을 위해 3200개 단어만 사용\n",
        "\n",
        "docs = docs[:3200]\n",
        "print(f\"Extracted {len(docs)} documents.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0XQX527xb-2",
        "outputId": "dc322186-a50f-4e8b-fef1-23eba06210ba"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted 3200 documents.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모듈 2 ▲ 토크나이저"
      ],
      "metadata": {
        "id": "YEE_yFrUwWVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uchars = sorted(set(''.join(docs)))   # 데이터셋에 등장한 고유 문자들\n",
        "BOS = len(uchars)                      # 특수 토큰: 시퀀스 시작/끝을 의미\n",
        "print(f\"Unique characters: {BOS}\")\n",
        "vocab_size = len(uchars) + 1           # 전체 어휘 크기\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmQSq540x0V5",
        "outputId": "3f2f84f3-78f7-4b9f-a8ea-fe35c2aa7e7c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique characters: 26\n",
            "27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "설명\n",
        "\n",
        "- 토큰화(Tokenization): 문자열 → 정수 시퀀스 변환.\n",
        "- 이 구현은 문자 단위(character-level) 토크나이저입니다. 'emma' → [4, 12, 12, 0] 형식.\n",
        "- 실제 GPT(GPT-4 등)는 BPE(Byte Pair Encoding) 방식을 사용하지만, 원리는 동일합니다.\n",
        "- BOS(Beginning of Sequence) 토큰은 문장의 시작과 끝을 알리는 특수 신호입니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "jsRNBnMCyjSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(uchars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Rj4Nep6ys91",
        "outputId": "fa1526f4-63fe-458c-b4c4-8ae9dc8e2a90"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 기존 설정값\n",
        "\n",
        "char_to_int = {char: i for i, char in enumerate(uchars)}\n",
        "\n",
        "\n",
        "# 실제 토큰화 함수 (Encode)\n",
        "# 이제 'emma'라는 단어를 넣으면 숫자로 짠! 하고 변하는 함수입니다.\n",
        "def encode_with_bos(text):\n",
        "    # 1. 텍스트를 숫자로 변환\n",
        "    tokens = [char_to_int[c] for c in text]\n",
        "\n",
        "    # 2. 앞뒤를 BOS(26)로 감싸기\n",
        "    # [26] + [4, 12, 12, 0] + [26] 형태가 됩니다.\n",
        "    return [BOS] + tokens + [BOS]\n",
        "\n",
        "# --- 실행 ---\n",
        "test_word = \"emma\"\n",
        "encoded = encode_with_bos(test_word)\n",
        "\n",
        "print(f\"원본: {test_word}\")\n",
        "print(f\"BOS로 감싼 토큰화 결과: {encoded}\")\n",
        "# 출력 결과: [26, 4, 12, 12, 0, 26]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFnuKrSi01wo",
        "outputId": "f9192820-88af-48d0-a4a9-a39fecd30779"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "원본: emma\n",
            "BOS로 감싼 토큰화 결과: [26, 4, 12, 12, 0, 26]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "'a' → 0, 'b' → 1, ..., 'z' → 25, BOS → 26\n",
        "\n",
        "\"emma\" → [26, 4, 12, 12, 0, 26]  (BOS로 감싸기)"
      ],
      "metadata": {
        "id": "l7MqtBC2yvcy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "마이크로GPT는 거대한 웹 데이터 대신 약 3만2000개의 영어 이름을 학습 데이터로 사용한다. 각 이름을 하나의 ‘문서(document)’로 간주하고, 이 이름들에 담긴 글자 배열의 통계적 패턴을 학습해 새로운 이름을 만들어낸다.\n",
        "\n",
        "학습이 끝난 뒤에는 kamon, karia, alerin, anton처럼 실제로 존재할 법한 새로운 이름을 생성할 수 있다. 이는 GPT가 본질적으로 ‘문서를 이어 쓰는(document completion) 모델’이라는 점을 보여준다. 우리가 챗봇과 주고받는 대화도 모델의 관점에서는 하나의 긴 문서를 계속 이어가는 과정에 불과하기 때문이다.\n",
        "\n",
        "실제 상용 모델들이 BPE(Byte Pair Encoding)와 같은 서브워드 기반 토크나이저를 사용하는 것과 달리, 마이크로GPT는 훨씬 단순한 방식을 택했다. 데이터에 등장하는 알파벳 문자(a~z) 각각에 정수 ID를 부여하고, 여기에 문서의 시작을 나타내는 BOS(Beginning of Sequence) 토큰을 더해 총 27개의 어휘만 사용한다.\n",
        "\n",
        "이렇게 변환된 텍스트는 숫자의 나열(정수 시퀀스)이 되고, 신경망은 이 숫자들을 입력으로 받아 다음에 올 문자의 확률을 예측하는 방식으로 학습을 진행한다.\n"
      ],
      "metadata": {
        "id": "9k52FTGryZc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 문자를 숫자로 (Character to Index)\n",
        "char_to_int = {char: i for i, char in enumerate(uchars)}\n",
        "\n",
        "# 숫자를 문자로 (Index to Character)\n",
        "int_to_char = {i: char for i, char in enumerate(uchars)}\n",
        "\n",
        "# BOS(문장 시작/끝) 토큰 추가 (예: 가장 마지막 번호 부여)\n",
        "BOS_TOKEN = len(uchars)\n",
        "int_to_char[BOS_TOKEN] = \"[BOS]\"\n",
        "\n",
        "print(char_to_int)\n",
        "print(int_to_char)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVJhIwowzAkh",
        "outputId": "78cb5a9b-1d23-4cea-e4a9-ffd13b1c530a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 9, 'k': 10, 'l': 11, 'm': 12, 'n': 13, 'o': 14, 'p': 15, 'q': 16, 'r': 17, 's': 18, 't': 19, 'u': 20, 'v': 21, 'w': 22, 'x': 23, 'y': 24, 'z': 25}\n",
            "{0: 'a', 1: 'b', 2: 'c', 3: 'd', 4: 'e', 5: 'f', 6: 'g', 7: 'h', 8: 'i', 9: 'j', 10: 'k', 11: 'l', 12: 'm', 13: 'n', 14: 'o', 15: 'p', 16: 'q', 17: 'r', 18: 's', 19: 't', 20: 'u', 21: 'v', 22: 'w', 23: 'x', 24: 'y', 25: 'z', 26: '[BOS]'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모듈 3 ▲ 자동 미분(Autograd) 엔진\n",
        "\n",
        "- 딥러닝의 심장이라고 할 수 있는 자동 미분(Autograd) 엔진의 핵심 원리를 구현한 클래스입니다.\n",
        "- 안드레이 카파시(Andrej Karpathy)의 micrograd 구조로, 연산 그래프를 따라 미분값(Gradient)이 어떻게 흐르는지 보여주는 다음 작동방식을 따른다.\n",
        "\n",
        "> **순전파(Forward):**\n",
        "\n",
        ">> 각 연산(+, * 등)이 수행될 때 결과값(data)을 계산하고, 나중에 미분에 써먹을 자식 노드와 로컬 미분값을 미리 저장\n",
        "\n",
        "> **역전파(Backward):**\n",
        "\n",
        ">> 결과값(보통 Loss)에서부터 시작해 거꾸로 내려오며, 저장해둔 로컬 미분값들을 곱해나가며 각 변수가 결과에 얼마나 기여했는지(grad)를 계산\n",
        "\n",
        "- 이 코드는 PyTorch의 Tensor 객체가 내부적으로 작동하는 방식을 가장 직관적으로 보여주는 모델입니다."
      ],
      "metadata": {
        "id": "-na2U9jJzAta"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***backward() 이해하기***\n",
        "\n",
        "우리가 계산하려는 식은 다음과 같습니다.\n",
        "- 입력노드 : $a = 2, b = 3, c = 10$\n",
        "- 중간노드 : $d = a \\times b$ (중간 노드)\n",
        "- 최종노드 : $L = d + c$ (최종 결과값, Loss라고 가정)"
      ],
      "metadata": {
        "id": "qcauhFIWuKw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# 1. 연산 함수 정의 (Node 생성)\n",
        "def create_node(data, children=(), local_grads=()):\n",
        "    return {\n",
        "        'data': data,\n",
        "        'grad': 0.0,\n",
        "        'children': children,\n",
        "        'local_grads': local_grads\n",
        "    }"
      ],
      "metadata": {
        "id": "rUlgZviux0bp"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 초기 입력값 설정\n",
        "a = create_node(2.0)\n",
        "b = create_node(3.0)\n",
        "c = create_node(10.0)\n",
        "print(a, b, c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sgzOJ6Qzcao",
        "outputId": "1fe93e57-7de5-41e2-d79b-282e0b5eb7e3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'data': 2.0, 'grad': 0.0, 'children': (), 'local_grads': ()} {'data': 3.0, 'grad': 0.0, 'children': (), 'local_grads': ()} {'data': 10.0, 'grad': 0.0, 'children': (), 'local_grads': ()}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add(x, y):\n",
        "    # x + y의 미분은 각각 1, 1\n",
        "    return create_node(x['data'] + y['data'], (x, y), (1.0, 1.0))\n",
        "\n",
        "def mul(x, y):\n",
        "    # x * y의 미분은 각각 y, x\n",
        "    return create_node(x['data'] * y['data'], (x, y), (y['data'], x['data']))\n"
      ],
      "metadata": {
        "id": "quMWujKmzaeK"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 순전파 (Forward Pass)\n",
        "d = mul(a, b)  # 2 * 3 = 6\n",
        "L = add(d, c)  # 6 + 10 = 16\n",
        "\n",
        "print(f'최종결과 d: {d[\"data\"]}, 최종결과 L: {L[\"data\"]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAEgUj22zhSA",
        "outputId": "7cc85854-20e4-4c62-d615-2730f9fa350e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "최종결과 d: 6.0, 최종결과 L: 16.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### build_topo(L)을 호출하면 함수는 **자신을 추가하기 전에 자식들을 먼저 끝까지 방문\"**, 이 과정을 단계별로 따라가 보면,\n",
        "\n",
        "- L 방문: L의 자식인 d와 c를 확인합니다.\n",
        "- d 방문 (L의 첫 번째 자식): d의 자식인 a와 b를 확인합니다.\n",
        "- a 방문: 자식이 없으므로 topo 리스트에 a 추가. [a]\n",
        "- b 방문: 자식이 없으므로 topo 리스트에 b 추가. [a, b]\n",
        "- 자식 방문이 끝났으므로 자신(d) 추가. [a, b, d]\n",
        "- c 방문 (L의 두 번째 자식): 자식이 없으므로 topo 리스트에 c 추가. [a, b, d, c]\n",
        "- L 마무리: 모든 자식(d, c)을 방문했으므로 마지막으로 자신(L) 추가.\n",
        "\n",
        "#### 최종 결과: topo = [a, b, d, c, L]"
      ],
      "metadata": {
        "id": "d1Fs0MAs0QbI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 2. 위상 정렬 함수 (Topological Sort)\n",
        "def build_topo(node, visited, topo):\n",
        "    if id(node) not in visited:\n",
        "        visited.add(id(node))\n",
        "        for child in node['children']:\n",
        "            build_topo(child, visited, topo)\n",
        "        topo.append(node)\n",
        "\n",
        "# 3. 역전파 실행 함수\n",
        "def backward(output_node):\n",
        "    topo = []\n",
        "    visited = set()\n",
        "    build_topo(output_node, visited, topo)\n",
        "\n",
        "    # 출력 노드의 미분값은 항상 1\n",
        "    output_node['grad'] = 1.0\n",
        "\n",
        "    # 역순으로 순회하며 체인 룰 적용\n",
        "    for node in reversed(topo):\n",
        "        for child, local_grad in zip(node['children'], node['local_grads']):\n",
        "            child['grad'] += local_grad * node['grad']"
      ],
      "metadata": {
        "id": "CbLlEE_CzgZh"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 역전파 (Backward Pass)\n",
        "\n",
        "backward(L) # 여기서 어떤 일이 벌어질까요?"
      ],
      "metadata": {
        "id": "PSWzNJIAx4op"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 결과 확인\n",
        "print(f\"최종 결과 L: {L['data']}\") # 16.0\n",
        "print(f\"a의 미분값 (dL/da): {a['grad']}\") # 3.0 (b의 값)\n",
        "print(f\"b의 미분값 (dL/db): {b['grad']}\") # 2.0 (a의 값)\n",
        "print(f\"c의 미분값 (dL/dc): {c['grad']}\") # 1.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7XfCPZkz-6p",
        "outputId": "177c63dc-9572-4ff6-a7ee-76866900856c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "최종 결과 L: 16.0\n",
            "a의 미분값 (dL/da): 3.0\n",
            "b의 미분값 (dL/db): 2.0\n",
            "c의 미분값 (dL/dc): 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "함수를 사용하여, 간단한 선형 회귀(Linear Regression) 모델을 학습시키는 과정\n",
        "- $y = w \\cdot x + b$라는 아주 단순한 식에서,\n",
        "- 데이터 $(x=2.0, y=5.0)$가 주어졌을 때 정답에 가까워지도록 가중치 $w$와 편향 $b$를 업데이트하는 과정을 담고 있습니다."
      ],
      "metadata": {
        "id": "DXiKjs821WCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. 데이터 및 파라미터 초기화\n",
        "x_data = create_node(2.0)     # 입력값 x\n",
        "y_true = create_node(5.0)     # 실제 정답 y\n",
        "\n",
        "w = create_node(0.5)          # 가중치 w (초기값)\n",
        "b = create_node(0.0)          # 편향 b (초기값)\n",
        "\n",
        "learning_rate = 0.01\n",
        "\n",
        "\n",
        "def sub(x, y):\n",
        "    # x - y -> 미분값 (1, -1)\n",
        "    return create_node(x['data'] - y['data'], (x, y), (1.0, -1.0))\n",
        "\n",
        "def pow_2(x):\n",
        "    # x^2 -> 미분값 2*x\n",
        "    return create_node(x['data']**2, (x,), (2.0 * x['data'],))\n",
        "\n",
        "\n",
        "loss_list = []\n",
        "# 4. 학습 루프 (50 Epochs)\n",
        "for i in range(50):\n",
        "    # --- [Forward Pass] 연산 그래프 구축 ---\n",
        "    # y_pred = w * x + b\n",
        "    wx = mul(w, x_data)\n",
        "    y_pred = add(wx, b)\n",
        "\n",
        "    # loss = (y_pred - y_true)^2\n",
        "    error = sub(y_pred, y_true)\n",
        "    loss = pow_2(error)\n",
        "    loss_list.append(loss)\n",
        "\n",
        "    # --- [Gradient Reset] 미분값 초기화 ---\n",
        "    # 중요: 이전 루프의 미분값이 남아있으면 안 되므로 재귀적으로 초기화하거나 수동 초기화\n",
        "    # 여기서는 학습 대상인 w, b만 초기화해도 결과에 영향을 줌\n",
        "    w['grad'] = 0.0\n",
        "    b['grad'] = 0.0\n",
        "    wx['grad'] = 0.0\n",
        "    y_pred['grad'] = 0.0\n",
        "    error['grad'] = 0.0\n",
        "    # (사실 모든 노드를 순회하며 0으로 만드는 것이 가장 정확합니다)\n",
        "\n",
        "    # --- [Backward Pass] ---\n",
        "    backward(loss)\n",
        "\n",
        "    # --- [Update] 파라미터 업데이트 ---\n",
        "    w['data'] -= learning_rate * w['grad']\n",
        "    b['data'] -= learning_rate * b['grad']\n",
        "\n",
        "    if (i + 1) % 10 == 0:\n",
        "        print(f\"Epoch {i+1:2d} | Loss: {loss['data']:.4f} | w: {w['data']:.4f}, b: {b['data']:.4f} | Pred: {y_pred['data']:.4f}\")\n",
        "\n",
        "print(\"-\" * 50)\n",
        "print(f\"최종 예측값: {y_pred['data']:.4f} (목표: 5.0)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgEIBo221WNn",
        "outputId": "40b5248d-caff-4d72-8738-4eac40e5fc08"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 | Loss: 2.4015 | w: 1.5421, b: 0.5211 | Pred: 3.4503\n",
            "Epoch 20 | Loss: 0.2920 | w: 1.9055, b: 0.7027 | Pred: 4.4597\n",
            "Epoch 30 | Loss: 0.0355 | w: 2.0322, b: 0.7661 | Pred: 4.8116\n",
            "Epoch 40 | Loss: 0.0043 | w: 2.0764, b: 0.7882 | Pred: 4.9343\n",
            "Epoch 50 | Loss: 0.0005 | w: 2.0918, b: 0.7959 | Pred: 4.9771\n",
            "--------------------------------------------------\n",
            "최종 예측값: 4.9771 (목표: 5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 'i'는 0부터 시작하여 len(loss_list)-1까지의 값을 가지므로, x축은 range(len(loss_list))로 설정합니다.\n",
        "steps = range(len(loss_list))\n",
        "# loss_list에서 각 Value 객체의 'data' 속성만 추출하여 플롯합니다.\n",
        "plt.plot(steps, [l['data'] for l in loss_list])\n",
        "plt.xlabel(\"Step (i): Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Loss over Training Steps\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "1zZWuI521fvf",
        "outputId": "f2618107-4967-48a6-8386-5172a77421bd"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Value 클래스를 사용하여, 간단한 선형 회귀(Linear Regression) 모델을 학습시키는 과정\n",
        "- $y = w \\cdot x + b$라는 아주 단순한 식에서,\n",
        "- 데이터 $(x=2.0, y=5.0)$가 주어졌을 때 정답에 가까워지도록 가중치 $w$와 편향 $b$를 업데이트하는 과정을 담고 있습니다."
      ],
      "metadata": {
        "id": "qE9F6hAK2eb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "class Value:\n",
        "    # 메모리 최적화: 인스턴스 변수를 고정하여 관리 (속도 향상 및 메모리 절감)\n",
        "    # __slots__는 파이썬에서 메모리 사용량을 줄이고 속도를 높이기 위해 사용하는 특수 변수\n",
        "    __slots__ = ('data', 'grad', '_children', '_local_grads')\n",
        "\n",
        "    def __init__(self, data, children=(), local_grads=()):\n",
        "        self.data = data                # 순전파(Forward) 시 계산된 실제 값 (스칼라)\n",
        "        self.grad = 0                   # 역전파(Backward) 시 계산될 손실 함수에 대한 이 노드의 미분값\n",
        "        self._children = children       # 이 노드를 만든 부모 노드들 (연산 그래프 추적용)\n",
        "        self._local_grads = local_grads # 현재 연산에서 각 자식에 대한 국소 미분값 (로컬 그래디언트)\n",
        "\n",
        "    # 더하기 연산 (a + b)\n",
        "    def __add__(self, other):\n",
        "        # 숫자가 들어오면 Value 객체로 변환\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        # 결과값 계산 및 로컬 미분값 (1, 1) 저장 (x+y를 x로 미분하면 1, y로 미분하면 1)\n",
        "        return Value(self.data + other.data, (self, other), (1, 1))\n",
        "\n",
        "    # 곱하기 연산 (a * b)\n",
        "    def __mul__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        # 로컬 미분값: x*y를 x로 미분하면 y, y로 미분하면 x\n",
        "        return Value(self.data * other.data, (self, other), (other.data, self.data))\n",
        "\n",
        "    # 거듭제곱 연산 (a ** n)\n",
        "    def __pow__(self, other):\n",
        "        # 로컬 미분값: x^n을 x로 미분하면 n * x^(n-1)\n",
        "        return Value(self.data**other, (self,), (other * self.data**(other-1),))\n",
        "\n",
        "    def log(self):\n",
        "        # log(x)를 x로 미분하면 1/x\n",
        "        return Value(math.log(self.data), (self,), (1/self.data,))\n",
        "\n",
        "    def exp(self):\n",
        "        # e^x를 x로 미분하면 e^x\n",
        "        return Value(math.exp(self.data), (self,), (math.exp(self.data),))\n",
        "\n",
        "    def relu(self):\n",
        "        # ReLU 미분: x > 0 이면 1, 아니면 0\n",
        "        return Value(max(0, self.data), (self,), (float(self.data > 0),))\n",
        "\n",
        "    # --- 연산자 오버로딩 (편의 기능) ---\n",
        "    def __neg__(self): return self * -1                # -self\n",
        "    def __radd__(self, other): return self + other      # other + self\n",
        "    def __sub__(self, other): return self + (-other)   # self - other\n",
        "    def __rsub__(self, other): return other + (-self)  # other - self\n",
        "    def __rmul__(self, other): return self * other      # other * self\n",
        "    def __truediv__(self, other): return self * other**-1 # self / other\n",
        "    def __rtruediv__(self, other): return other * self**-1 # other / self\n",
        "\n",
        "    # 역전파(Backpropagation) 엔진\n",
        "    def backward(self):\n",
        "        topo = []      # 위상 정렬된 노드 리스트\n",
        "        visited = set()\n",
        "\n",
        "        # 1. 위상 정렬 (Topological Sort): 그래프를 끝에서부터 거꾸로 순회하기 위한 순서 정하기\n",
        "        def build_topo(v):\n",
        "            if v not in visited:\n",
        "                visited.add(v)\n",
        "                for child in v._children:\n",
        "                    build_topo(child)\n",
        "                topo.append(v) # 자식들을 먼저 다 방문한 후 자신을 추가\n",
        "\n",
        "        build_topo(self)\n",
        "\n",
        "        # 2. 미분 시작\n",
        "        self.grad = 1 # 출력(Loss)에 대한 자기 자신의 미분값은 항상 1\n",
        "\n",
        "        # 3. 체인 룰(Chain Rule) 적용: 정렬된 리스트를 역순으로 순회\n",
        "        for v in reversed(topo):\n",
        "            for child, local_grad in zip(v._children, v._local_grads):\n",
        "                # 자식의 미분값 = (현재 노드의 미분값) * (현재 연산의 로컬 미분값)\n",
        "                child.grad += local_grad * v.grad"
      ],
      "metadata": {
        "id": "HaeLEtE42CXG"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w = Value(0.5)\n",
        "print(w.data)\n",
        "print(w.grad)\n",
        "print(w._children) # 이 노드를 만든 부모 노드들 (연산 그래프 추적용)\n",
        "print(w._local_grads) # 현재 연산에서 각 자식에 대한 국소 미분값 (로컬 그래디언트)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fw-P1AZ2w0m",
        "outputId": "b7583d9a-89f6-4034-e1b1-a55d28a8d813"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5\n",
            "0\n",
            "()\n",
            "()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 초기 파라미터 설정 (w=0.5, b=0.0으로 시작)\n",
        "w = Value(0.5); b = Value(0.0)\n",
        "\n",
        "# 학습 데이터 (입력 2.0일 때 정답은 5.0이라고 가정)\n",
        "x = 2.0\n",
        "target = 5.0\n",
        "\n",
        "pred_list = []\n",
        "loss_list = []\n",
        "\n",
        "# 학습 횟수 (Epochs)\n",
        "for i in range(50):\n",
        "    # --- [순전파 (Forward Pass)] ---\n",
        "    # 예측값 계산: y = w * x + b\n",
        "    pred = w * x + b\n",
        "    pred_list.append(pred.data)\n",
        "    # 손실 함수 계산: (예측값 - 정답)^2 (Mean Squared Error 방식)\n",
        "    loss = (pred - target)**2\n",
        "    loss_list.append(loss.data)\n",
        "\n",
        "    # --- [역전파 (Backward Pass)] ---\n",
        "    # 기존 미분값 초기화 (매 단계 새로 계산해야 함)\n",
        "    w.grad = 0; b.grad = 0\n",
        "\n",
        "    # 전체 그래프를 거슬러 올라가며 미분값(grad) 계산\n",
        "    loss.backward()\n",
        "\n",
        "    # --- [파라미터 업데이트 (Optimizer)] ---\n",
        "    # 학습률(Learning Rate)을 곱해 미분 반대 방향으로 이동\n",
        "    learning_rate = 0.01\n",
        "    w.data -= learning_rate * w.grad\n",
        "    b.data -= learning_rate * b.grad\n",
        "\n",
        "    if i % 5 == 0:\n",
        "        print(f\"Step {i}: Loss = {loss.data:.4f}, Pred = {pred.data:.4f}\")\n",
        "\n",
        "\n",
        "print(f'pred_list: {pred_list}')\n",
        "print(f'loss_list: {loss_list}')\n",
        "print(f\"\\n최종 결과 -> w: {w.data:.2f}, b: {b.data:.2f}\")\n",
        "print(f\"최종 예측값: {w.data * x + b.data:.2f} (정답 5.0에 근접!)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4RVO9A32H2G",
        "outputId": "c19b0afa-c7c4-4cb0-c1e1-22423fb6cedc"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0: Loss = 16.0000, Pred = 1.0000\n",
            "Step 5: Loss = 5.5789, Pred = 2.6380\n",
            "Step 10: Loss = 1.9452, Pred = 3.6053\n",
            "Step 15: Loss = 0.6783, Pred = 4.1764\n",
            "Step 20: Loss = 0.2365, Pred = 4.5137\n",
            "Step 25: Loss = 0.0825, Pred = 4.7128\n",
            "Step 30: Loss = 0.0288, Pred = 4.8304\n",
            "Step 35: Loss = 0.0100, Pred = 4.8999\n",
            "Step 40: Loss = 0.0035, Pred = 4.9409\n",
            "Step 45: Loss = 0.0012, Pred = 4.9651\n",
            "pred_list: [1.0, 1.4000000000000001, 1.76, 2.084, 2.3756, 2.63804, 2.8742360000000002, 3.0868124000000003, 3.27813116, 3.4503180440000003, 3.6052862396000003, 3.74475761564, 3.8702818540760004, 3.9832536686684, 4.084928301801559, 4.176435471621404, 4.258791924459263, 4.332912732013337, 4.399621458812003, 4.459659312930802, 4.513693381637722, 4.56232404347395, 4.606091639126555, 4.6454824752139, 4.68093422769251, 4.712840804923259, 4.741556724430933, 4.767401051987839, 4.790660946789056, 4.81159485211015, 4.830435366899135, 4.847391830209221, 4.862652647188299, 4.876387382469469, 4.888748644222522, 4.89987377980027, 4.909886401820243, 4.918897761638219, 4.927007985474397, 4.934307186926957, 4.940876468234262, 4.946788821410836, 4.952109939269752, 4.956898945342777, 4.961209050808499, 4.965088145727649, 4.968579331154884, 4.971721398039396, 4.974549258235457, 4.977094332411911]\n",
            "loss_list: [16.0, 12.959999999999997, 10.497600000000002, 8.503055999999999, 6.887475360000001, 5.578855041599999, 4.518872583695999, 3.660286792793759, 2.9648323021629452, 2.401514164751985, 1.9452264734491078, 1.5756334434937775, 1.2762630892299591, 1.0337731022762675, 0.8373562128437779, 0.6782585324034596, 0.5493894112468024, 0.4450054231099094, 0.3604543927190271, 0.2919680581024125, 0.23649412706295403, 0.19156024292099277, 0.15516379676600386, 0.12568267538046327, 0.10180296705817513, 0.08246040331712201, 0.06679292668686863, 0.05410227061636381, 0.043822839199254456, 0.03549649975139628, 0.028752164798631078, 0.023289253486891117, 0.01886429532438183, 0.015280079212749391, 0.012376864162326967, 0.01002525997148481, 0.008120460576902695, 0.006577573067291211, 0.005327834184505855, 0.004315545689449766, 0.0034955920084541953, 0.0028314295268479076, 0.002293457916746856, 0.0018577009125649076, 0.0015047377391776164, 0.0012188375687338568, 0.0009872584306744407, 0.0007996793288462921, 0.0006477402563654695, 0.0005246696076560505]\n",
            "\n",
            "최종 결과 -> w: 2.09, b: 0.80\n",
            "최종 예측값: 4.98 (정답 5.0에 근접!)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 'i'는 0부터 시작하여 len(loss_list)-1까지의 값을 가지므로, x축은 range(len(loss_list))로 설정합니다.\n",
        "steps = range(len(loss_list))\n",
        "plt.plot(steps, loss_list)\n",
        "plt.xlabel(\"Step (i): Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Loss over Training Steps\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "4UmFmr-c2QW2",
        "outputId": "760f152b-2814-4bdf-9dc0-76a19685bc62"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjMAAAHHCAYAAABKudlQAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAVNFJREFUeJzt3Xd8U/X+P/DXSZom3YPu0gVllj2KbBBKRWSKojgYXr0qKIg/vKIXBBwofvVyUa7gdeAARVC4TqRsEBAKlE1ZpZROSkdKR5o2n98fpYHYQUeSk7Sv5+ORR5NPzjl5990CL875nHMkIYQAERERkZ1SyF0AERERUWMwzBAREZFdY5ghIiIiu8YwQ0RERHaNYYaIiIjsGsMMERER2TWGGSIiIrJrDDNERERk1xhmiIiIyK4xzBBRk7d69WpIkoTLly/Xe92dO3dCkiTs3LnT7HURkXkwzBDVU+U/jPHx8XKXYveGDBkCSZLu+Fi4cKHcpcrmxIkTmDhxIsLCwqDRaBAcHIyYmBh88MEHJsu99dZb2LRpkzxFEslM4r2ZiOpn9erVmDZtGg4dOoRevXrJXY5di4uLQ2ZmpvH1oUOHsHz5crzyyivo0KGDcbxLly7o0qVLgz+nvLwcer0earUakiTVa12DwYDS0lI4OjpCobDu///27duHoUOHIjQ0FFOmTEFAQABSUlJw4MABXLx4ERcuXDAu6+rqiokTJ2L16tVWrZHIFjjIXQARNX2FhYVwcXGpMh4TE2PyWqPRYPny5YiJicGQIUPqvb2aKJVKKJXKOi9/O4VCAY1G06B1G+vNN9+Eh4cHDh06BE9PT5P3srKyZKmJyBbxMBORhRw9ehQjR46Eu7s7XF1dMWzYMBw4cMBkGb1ej0WLFqFNmzbQaDRo0aIFBgwYgLi4OOMyGRkZmDZtGlq2bAm1Wo3AwECMHTu2TvM/tm/fjoEDB8LFxQWenp4YO3Yszpw5Y3x/w4YNkCQJu3btqrLuqlWrIEkSTp48aRw7e/YsJk6cCG9vb2g0GvTq1Qs//vijyXqVh+F27dqFZ599Fn5+fmjZsmVd21bFwoULIUkSTp8+jcmTJ8PLywsDBgwAABw/fhxTp05Fq1atoNFoEBAQgOnTp+P69evV1nR7z8LDw3Hfffdh7969iI6OhkajQatWrfDll1+arFvdnJkhQ4agU6dOOH36NIYOHQpnZ2cEBwdj6dKlVepPTk7GmDFj4OLiAj8/P7zwwgv4/fff6zQP5+LFi4iKiqoSZADAz8/P+FySJBQWFuKLL74wHpqbOnWq8f3U1FRMnz4d/v7+UKvViIqKwmeffVbt97lu3Tq88sorCAgIgIuLC8aMGYOUlBSTZc+fP4/7778fAQEB0Gg0aNmyJR566CHk5+fX+v0QWQr3zBBZwKlTpzBw4EC4u7vjpZdegkqlwqpVqzBkyBDs2rULffr0AVDxD/WSJUvwt7/9DdHR0dBqtYiPj8eRI0eMey3uv/9+nDp1Cs899xzCw8ORlZWFuLg4XLlyBeHh4TXWsHXrVowcORKtWrXCwoULUVxcjA8++AD9+/fHkSNHEB4ejlGjRsHV1RXfffcdBg8ebLL+unXrEBUVhU6dOhm/p/79+yM4OBgvv/wyXFxc8N1332HcuHH4/vvvMX78eJP1n332Wfj6+mLBggUoLCxsdE8feOABtGnTBm+99RYqj47HxcXh0qVLmDZtGgICAnDq1Cl8/PHHOHXqFA4cOHDHQ0oXLlzAxIkT8cQTT2DKlCn47LPPMHXqVPTs2RNRUVG1rpubm4t77rkHEyZMwIMPPogNGzbgH//4Bzp37oyRI0cCqNiDdPfddyM9PR2zZs1CQEAA1q5dix07dtTpew4LC8P+/ftx8uRJ48+hOl999ZXxd+ipp54CALRu3RoAkJmZibvuuguSJGHmzJnw9fXFb7/9hieeeAJarRazZ8822dabb74JSZLwj3/8A1lZWVi2bBmGDx+OhIQEODk5obS0FLGxsdDpdHjuuecQEBCA1NRU/Pzzz8jLy4OHh0edvjcisxJEVC+ff/65ACAOHTpU4zLjxo0Tjo6O4uLFi8axtLQ04ebmJgYNGmQc69q1qxg1alSN28nNzRUAxLvvvlvvOrt16yb8/PzE9evXjWPHjh0TCoVCPP7448axhx9+WPj5+YmysjLjWHp6ulAoFGLx4sXGsWHDhonOnTuLkpIS45jBYBD9+vUTbdq0MY5V9mfAgAEm26yL9evXCwBix44dxrHXXntNABAPP/xwleWLioqqjH3zzTcCgNi9e3eVmpKSkoxjYWFhVZbLysoSarVavPjii8axHTt2VKlp8ODBAoD48ssvjWM6nU4EBASI+++/3zj23nvvCQBi06ZNxrHi4mLRvn37KtuszpYtW4RSqRRKpVL07dtXvPTSS+L3338XpaWlVZZ1cXERU6ZMqTL+xBNPiMDAQJGdnW0y/tBDDwkPDw9jDyu/z+DgYKHVao3LfffddwKA+Pe//y2EEOLo0aMCgFi/fn2ttRNZEw8zEZlZeXk5tmzZgnHjxqFVq1bG8cDAQEyePBl79+6FVqsFAHh6euLUqVM4f/58tdtycnKCo6Mjdu7cidzc3DrXkJ6ejoSEBEydOhXe3t7G8S5duiAmJga//vqrcWzSpEnIysoyOeSxYcMGGAwGTJo0CQCQk5OD7du348EHH0RBQQGys7ORnZ2N69evIzY2FufPn0dqaqpJDU8++WSD56lU5+mnn64y5uTkZHxeUlKC7Oxs3HXXXQCAI0eO3HGbHTt2xMCBA42vfX190a5dO1y6dOmO67q6uuLRRx81vnZ0dER0dLTJups3b0ZwcDDGjBljHNNoNHjyySfvuH2gYk7R/v37MWbMGBw7dgxLly5FbGwsgoODqxzeq44QAt9//z1Gjx4NIYTx55adnY3Y2Fjk5+dX6dPjjz8ONzc34+uJEyciMDDQ+DtTuefl999/R1FRUZ2+DyJLY5ghMrNr166hqKgI7dq1q/Jehw4dYDAYjHMQFi9ejLy8PLRt2xadO3fG3Llzcfz4cePyarUa77zzDn777Tf4+/tj0KBBWLp0KTIyMmqtITk5GQBqrCE7O9t46Oeee+6Bh4cH1q1bZ1xm3bp16NatG9q2bQug4nCMEALz58+Hr6+vyeO1114DUHVCakRExB17VR/VbS8nJwezZs2Cv78/nJyc4Ovra1yuLvM3QkNDq4x5eXnVKTi2bNmyymGsv66bnJyM1q1bV1kuMjLyjtuv1Lt3b/zwww/Izc3FwYMHMW/ePBQUFGDixIk4ffp0reteu3YNeXl5+Pjjj6v83KZNmwag6s+tTZs2Jq8lSUJkZKRxvlFERATmzJmDTz75BD4+PoiNjcWKFSs4X4ZkxTBDJKNBgwbh4sWL+Oyzz9CpUyd88skn6NGjBz755BPjMrNnz8a5c+ewZMkSaDQazJ8/Hx06dMDRo0fNUoNarca4ceOwceNGlJWVITU1FX/88YdxrwxQcXoyAPy///f/EBcXV+3jr/9A377XxByq296DDz6I//73v3j66afxww8/YMuWLdi8ebNJzbWpac+RqMMVKxqzbkM4Ojqid+/eeOutt/DRRx9Br9dj/fr1ta5T2YNHH320xp9b//79613Le++9h+PHj+OVV15BcXExnn/+eURFReHq1asN+t6IGosTgInMzNfXF87OzkhMTKzy3tmzZ6FQKBASEmIc8/b2xrRp0zBt2jTcuHEDgwYNwsKFC/G3v/3NuEzr1q3x4osv4sUXX8T58+fRrVs3vPfee/j666+rrSEsLAwAaqzBx8fH5NTmSZMm4YsvvsC2bdtw5swZCCFMwkzl4TKVSoXhw4fXsyOWkZubi23btmHRokVYsGCBcbymQ3ZyCAsLw+nTpyGEMNk7c/v1YRqi8vpG6enpxrHqJjv7+vrCzc0N5eXldf65/bV/QghcuHChynV+OnfujM6dO+Of//wn9u3bh/79+2PlypV444036vvtEDUa98wQmZlSqcSIESPwv//9z+RU4MzMTKxduxYDBgyAu7s7AFQ5hdjV1RWRkZHQ6XQAgKKiIpSUlJgs07p1a7i5uRmXqU5gYCC6deuGL774Anl5ecbxkydPYsuWLbj33ntNlh8+fDi8vb2xbt06rFu3DtHR0SaHdfz8/DBkyBCsWrXK5B/QSteuXau9KRZQuWfkr3tCli1bZvVaahIbG4vU1FST+S0lJSX473//W6f1d+zYUe2ensr5K7cfRnRxcTH5WQMVPbr//vvx/fffm5xiX6m6n9uXX36JgoIC4+sNGzYgPT3deIaWVqtFWVmZyTqdO3eGQqGo9XeSyJK4Z4aogT777DPjIY3bzZo1C2+88Qbi4uIwYMAAPPvss3BwcMCqVaug0+lMrkXSsWNHDBkyBD179oS3tzfi4+OxYcMGzJw5EwBw7tw5DBs2DA8++CA6duwIBwcHbNy4EZmZmXjooYdqre/dd9/FyJEj0bdvXzzxxBPGU7M9PDyq3B5ApVJhwoQJ+Pbbb1FYWIj/+7//q7K9FStWYMCAAejcuTOefPJJtGrVCpmZmdi/fz+uXr2KY8eONaCLDefu7m6cQ6TX6xEcHIwtW7YgKSnJqnXU5u9//zs+/PBDPPzww5g1axYCAwOxZs0a40X47nTq+HPPPYeioiKMHz8e7du3R2lpKfbt24d169YhPDzcOO8FAHr27ImtW7fi/fffR1BQECIiItCnTx+8/fbb2LFjB/r06YMnn3wSHTt2RE5ODo4cOYKtW7ciJyfH5DO9vb0xYMAATJs2DZmZmVi2bBkiIyONk5a3b9+OmTNn4oEHHkDbtm1RVlaGr776yhiciGQh12lURPaq8jTfmh4pKSlCCCGOHDkiYmNjhaurq3B2dhZDhw4V+/btM9nWG2+8IaKjo4Wnp6dwcnIS7du3F2+++abx1Nvs7GwxY8YM0b59e+Hi4iI8PDxEnz59xHfffVenWrdu3Sr69+8vnJychLu7uxg9erQ4ffp0tcvGxcUJAEKSJOP38FcXL14Ujz/+uAgICBAqlUoEBweL++67T2zYsKFKf2o7db0mtZ2afe3atSrLX716VYwfP154enoKDw8P8cADD4i0tDQBQLz22mtVavrrqdnVnRY/ePBgMXjwYOPrmk7NjoqKqrLulClTRFhYmMnYpUuXxKhRo4STk5Pw9fUVL774ovj+++8FAHHgwIFa+/Hbb7+J6dOni/bt2wtXV1fh6OgoIiMjxXPPPScyMzNNlj179qwYNGiQcHJyEgBMTtPOzMwUM2bMECEhIUKlUomAgAAxbNgw8fHHH1f5Pr/55hsxb9484efnJ5ycnMSoUaNEcnKyyfczffp00bp1a6HRaIS3t7cYOnSo2Lp1a63fC5El8d5MRERWtmzZMrzwwgu4evUqgoOD5S4HQMUVgIcOHYr169dj4sSJcpdDVC+cM0NEZEHFxcUmr0tKSrBq1Sq0adPGZoIMkb3jnBkiIguaMGECQkND0a1bN+Tn5+Prr7/G2bNnsWbNGrlLI2oyGGaIiCwoNjYWn3zyCdasWYPy8nJ07NgR3377rcmp70TUOJwzQ0RERHaNc2aIiIjIrjHMEBERkV1r8nNmDAYD0tLS4ObmdscLVBEREZFtEEKgoKAAQUFBUChq3/fS5MNMWlqayX1wiIiIyH6kpKSgZcuWtS7T5MOMm5sbgIpmVN4Px1z0ej22bNmCESNGQKVSmXXbVBX7bV3st3Wx39bFfltXQ/qt1WoREhJi/He8Nk0+zFQeWnJ3d7dImHF2doa7uzv/MFgB+21d7Ld1sd/WxX5bV2P6XZcpIpwATERERHaNYYaIiIjsGsMMERER2TWGGSIiIrJrDDNERERk1xhmiIiIyK4xzBAREZFdY5ghIiIiu8YwQ0RERHaNYYaIiIjsmqxhZvfu3Rg9ejSCgoIgSRI2bdpUZZkzZ85gzJgx8PDwgIuLC3r37o0rV65Yv1giIiKySbKGmcLCQnTt2hUrVqyo9v2LFy9iwIABaN++PXbu3Injx49j/vz50Gg0Vq6UiIiIbJWsN5ocOXIkRo4cWeP7r776Ku69914sXbrUONa6dWtrlHZH5QaBq7nFyNPJXQkREVHzZrN3zTYYDPjll1/w0ksvITY2FkePHkVERATmzZuHcePG1bieTqeDTncrYWi1WgAVd+zU6/Vmq2/p7+fw372XMShAgQfMuF2qWeXPz5w/R6oZ+21d7Ld1sd/W1ZB+12dZSQgh6l2VBUiShI0bNxqDSkZGBgIDA+Hs7Iw33ngDQ4cOxebNm/HKK69gx44dGDx4cLXbWbhwIRYtWlRlfO3atXB2djZbvfsyJay7pEQHTwOe7mAw23aJiIgIKCoqwuTJk5Gfnw93d/dal7XZMJOWlobg4GA8/PDDWLt2rXG5MWPGwMXFBd98802126luz0xISAiys7Pv2Iz6OHApB499Hg8fjcDul+6GSqUy27apenq9HnFxcYiJiWG/rYD9ti7227rYb+tqSL+1Wi18fHzqFGZs9jCTj48PHBwc0LFjR5PxDh06YO/evTWup1aroVarq4yrVCqz/sJGBlQ0NqcEgELJPwxWZO6fJdWO/bYu9tu62G/rqk+/6/NzsdnrzDg6OqJ3795ITEw0GT937hzCwsJkquoWfzcNNCoFDJCQllcidzlERETNlqx7Zm7cuIELFy4YXyclJSEhIQHe3t4IDQ3F3LlzMWnSJAwaNMg4Z+ann37Czp075Sv6JoVCQpi3MxIzb+Dy9UJEBnjIXRIREVGzJOuemfj4eHTv3h3du3cHAMyZMwfdu3fHggULAADjx4/HypUrsXTpUnTu3BmffPIJvv/+ewwYMEDOso1CvSsmFF++XiRzJURERM2XrHtmhgwZgjvNP54+fTqmT59upYrqJ7xFRZhJZpghIiKSjc3OmbEHlWGGe2aIiIjkwzDTCGEMM0RERLJjmGmEyj0zqXnFKC3jhfOIiIjkwDDTCH5uajgqBAwCSMnl3hkiIiI5MMw0giRJ8L15A+/L2YXyFkNERNRMMcw0kq+m4mysJIYZIiIiWTDMNJKPU8VXnp5NREQkD4aZRqrcM3P5OvfMEBERyYFhppF4mImIiEheDDONVDkBOC2vGLqycnmLISIiaoYYZhrJTQW4OCorTs/O4bwZIiIia2OYaSRJunUl4KRshhkiIiJrY5gxA+M9mjhvhoiIyOoYZszAuGeGZzQRERFZHcOMGXDPDBERkXwYZswgvIULAIYZIiIiOTDMmEGYd8VlgNPyS1Ci5+nZRERE1sQwYwbeLo5wUzsAAK7w9GwiIiKrYpgxA0mSEO5TcaiJVwImIiKyLoYZM6kMM5w3Q0REZF0MM2YSUXlGE0/PJiIisiqGGTPhYSYiIiJ5MMyYya3DTJwATEREZE0MM2YScfNaMxnaEhSX8vRsIiIia2GYMRMvF0d4OKkAcN4MERGRNTHMmBHPaCIiIrI+hhkzMt6j6TrnzRAREVkLw4wZ8R5NRERE1scwY0YRladnc84MERGR1TDMmBHnzBAREVkfw4wZVZ6enVWgQ6GuTOZqiIiImgdZw8zu3bsxevRoBAUFQZIkbNq0qcZln376aUiShGXLllmtvvrycFbBy5mnZxMREVmTrGGmsLAQXbt2xYoVK2pdbuPGjThw4ACCgoKsVFnD8UrARERE1uUg54ePHDkSI0eOrHWZ1NRUPPfcc/j9998xatQoK1XWcBEtXHD0Sh73zBAREVmJrGHmTgwGAx577DHMnTsXUVFRdVpHp9NBp9MZX2u1WgCAXq+HXq83a32V27t9uyFeGgDAxawCs39ec1ddv8ly2G/rYr+ti/22rob0uz7L2nSYeeedd+Dg4IDnn3++zussWbIEixYtqjK+ZcsWODs7m7M8o7i4OOPz3GwJgBIJF1Lx669XLPJ5zd3t/SbLY7+ti/22LvbbuurT76Kiuk/XsNkwc/jwYfz73//GkSNHIElSndebN28e5syZY3yt1WoREhKCESNGwN3d3aw16vV6xMXFISYmBipVxcTfkNR8fHn+T2iFGvfeO8Ssn9fcVddvshz227rYb+tiv62rIf2uPLJSFzYbZvbs2YOsrCyEhoYax8rLy/Hiiy9i2bJluHz5crXrqdVqqNXqKuMqlcpiv7C3bzsywAMAkH2jFCXlgJuGf0jMzZI/S6qK/bYu9tu62G/rqk+/6/Nzsdkw89hjj2H48OEmY7GxsXjssccwbdo0maq6M3eNCi1cHHG9sBTJ14vQKdhD7pKIiIiaNFnDzI0bN3DhwgXj66SkJCQkJMDb2xuhoaFo0aKFyfIqlQoBAQFo166dtUutl3AfF1wvLEVSdiHDDBERkYXJep2Z+Ph4dO/eHd27dwcAzJkzB927d8eCBQvkLKvReMNJIiIi65F1z8yQIUMghKjz8jXNk7E1ET4VZ03xhpNERESWx3szWQBvOElERGQ9DDMWYDzMdJ23NCAiIrI0hhkLqNwzk1NYivxiXl2SiIjIkhhmLMBV7QAf14pr3SRz3gwREZFFMcxYiHESMOfNEBERWRTDjIXcOj2b82aIiIgsiWHGQoxnNPEwExERkUUxzFhIxM0ww8NMRERElsUwYyG3Ts9mmCEiIrIkhhkLCb85ATivSI+8olKZqyEiImq6GGYsxNnRAf7uFadn81ATERGR5TDMWBAPNREREVkew4wFVYaZJJ6eTUREZDEMMxZUeXo2rwJMRERkOQwzFsSrABMREVkew4wFRfq5AQDOZ96AwSBkroaIiKhpYpixoPAWzlA7KFCsL8eVHM6bISIisgSGGQtyUCrQxt8VAHA2QytzNURERE0Tw4yFtQ9wBwCczSiQuRIiIqKmiWHGwtoHVMybOZvOMENERGQJDDMWdmvPDA8zERERWQLDjIW1D6zYM5OcU4Si0jKZqyEiImp6GGYszMdVDR9XRwgBnMu8IXc5RERETQ7DjBVUHmpK5KEmIiIis2OYsYJ2NycBn+EkYCIiIrNjmLGCyjOaEnl6NhERkdkxzFjB7Wc0CcHbGhAREZkTw4wVtPF3hUICcov0uFagk7scIiKiJoVhxgo0KiUifFwAAGd4qImIiMisGGasxHioKZ1nNBEREZkTw4yVcBIwERGRZcgaZnbv3o3Ro0cjKCgIkiRh06ZNxvf0ej3+8Y9/oHPnznBxcUFQUBAef/xxpKWlyVdwIxhPz2aYISIiMitZw0xhYSG6du2KFStWVHmvqKgIR44cwfz583HkyBH88MMPSExMxJgxY2SotPE6BFYcZrqYdQP6coPM1RARETUdDnJ++MiRIzFy5Mhq3/Pw8EBcXJzJ2Icffojo6GhcuXIFoaGh1ijRbII9neDiqERhaTmSsgvR1t9N7pKIiIiaBFnDTH3l5+dDkiR4enrWuIxOp4NOd+v0Z622YsKtXq+HXq83az2V26vrdtv6u+JoSj5OXc1FhLfGrLU0B/XtNzUO+21d7Ld1sd/W1ZB+12dZSdjIVdwkScLGjRsxbty4at8vKSlB//790b59e6xZs6bG7SxcuBCLFi2qMr527Vo4Ozubq9wGWXdJgX2ZCgwPNmB0KA81ERER1aSoqAiTJ09Gfn4+3N3da13WLvbM6PV6PPjggxBC4KOPPqp12Xnz5mHOnDnG11qtFiEhIRgxYsQdm9GQuuLi4hATEwOVSnXH5XP+vIJ9P59Fuasf7r23h1lraQ7q229qHPbbuthv62K/rash/a48slIXNh9mKoNMcnIytm/ffsdAolaroVarq4yrVCqL/cLWddtRwV4AgHOZhfzD0wiW/FlSVey3dbHf1sV+W1d9+l2fn4tNX2emMsicP38eW7duRYsWLeQuqVHa3Zz0m5pXjPxiHqclIiIyB1n3zNy4cQMXLlwwvk5KSkJCQgK8vb0RGBiIiRMn4siRI/j5559RXl6OjIwMAIC3tzccHR3lKrvBPJxVCPLQIC2/BOcyC9A73FvukoiIiOyerHtm4uPj0b17d3Tv3h0AMGfOHHTv3h0LFixAamoqfvzxR1y9ehXdunVDYGCg8bFv3z45y26Uyovn8bYGRERE5iHrnpkhQ4agtpOpbOREK7NqH+iOHYnXcJZXAiYiIjILm54z0xRV3qOJYYaIiMg8GGasrPLu2YkZBU1yzxMREZG1McxYWStfF6iUEm7oynA1t1jucoiIiOwew4yVqZQKtPZ1BVCxd4aIiIgah2FGBpV30D6bwTOaiIiIGothRgaVp2ef4Z4ZIiKiRmOYkUHlGU08zERERNR4DDMyqDyjKSm7ECX6cpmrISIism8MMzLwd1fD01mFcoPAhawbcpdDRERk1xhmZCBJEi+eR0REZCYMMzK5dfE8ntFERETUGAwzMuGeGSIiIvNgmJFJO4YZIiIis2CYkUlbfzdIEnCtQIfsGzq5yyEiIrJbDDMycVE7INTbGQCvN0NERNQYDDMy4rwZIiKixmOYkVHlGU1n03lGExERUUMxzMjIeFuDTO6ZISIiaiiGGRm1D6y81kwByg1C5mqIiIjsE8OMjEK9naFRKaArMyD5eqHc5RAREdklhhkZKRUS2vlzEjAREVFjMMzIjBfPIyIiahyGGZnxjCYiIqLGYZiRWftAntFERETUGAwzMqvcM5N8vQjaEr3M1RAREdkfhhmZebs4Gm9rcCwlT95iiIiI7BDDjA3oHuoJADh6JU/WOoiIiOwRw4wN6B7iCQA4eiVX3kKIiIjsEMOMDege6gUAOJqSByF4JWAiIqL6YJixAR0C3aF2UCCvSI+kbF4JmIiIqD4YZmyAo4MCnYM9AHDeDBERUX3JGmZ2796N0aNHIygoCJIkYdOmTSbvCyGwYMECBAYGwsnJCcOHD8f58+flKdbCjJOAUzhvhoiIqD5kDTOFhYXo2rUrVqxYUe37S5cuxfLly7Fy5Ur8+eefcHFxQWxsLEpKSqxcqeUZ581wzwwREVG9OMj54SNHjsTIkSOrfU8IgWXLluGf//wnxo4dCwD48ssv4e/vj02bNuGhhx6yZqkWV7ln5mxGAYpKy+DsKOuPhoiIyG7Y7L+YSUlJyMjIwPDhw41jHh4e6NOnD/bv319jmNHpdNDpdMbXWm3FPY/0ej30evNeYbdye+bYro+zAwLc1cjQ6nDk8nX0ifBu9DabGnP2m+6M/bYu9tu62G/raki/67OszYaZjIwMAIC/v7/JuL+/v/G96ixZsgSLFi2qMr5lyxY4Ozubt8ib4uLizLKdAAcFMqDAuq1/4nowT9Guibn6TXXDflsX+21d7Ld11affRUVFdV7WZsNMQ82bNw9z5swxvtZqtQgJCcGIESPg7u5u1s/S6/WIi4tDTEwMVCpVo7eX7nEZCZvPocQlEPfe263xBTYx5u431Y79ti7227rYb+tqSL8rj6zUhc2GmYCAAABAZmYmAgMDjeOZmZno1q1bjeup1Wqo1eoq4yqVymK/sObadq/wFgCAhKv5cHBwgCRJjd5mU2TJnyVVxX5bF/ttXey3ddWn3/X5udjsdWYiIiIQEBCAbdu2Gce0Wi3+/PNP9O3bV8bKLKdTsAdUSgnXCnS4mlssdzlERER2QdY9Mzdu3MCFCxeMr5OSkpCQkABvb2+EhoZi9uzZeOONN9CmTRtERERg/vz5CAoKwrhx4+Qr2oI0KiU6Brrj2NV8HE3JQ4i3Zeb4EBERNSWyhpn4+HgMHTrU+LpyrsuUKVOwevVqvPTSSygsLMRTTz2FvLw8DBgwAJs3b4ZGo5GrZIvrHupVEWau5GJM1yC5yyEiIrJ5soaZIUOG1HpjRUmSsHjxYixevNiKVcmre6gnVu/jxfOIiIjqymbnzDRX3UMqrgR8Ki0fJfpymashIiKyfQwzNibE2wk+ro7QlwucSqv7aWlERETNFcOMjZEkCd1CKu/TxJtOEhER3QnDjA26dQftPFnrICIisgcMMzbIGGaSuWeGiIjoThhmbFDXlp5QSEBafgky8kvkLoeIiMimMczYIBe1A9oFVNxHKiGFe2eIiIhqwzBjo4yHmni9GSIioloxzNio7iGeAIAjPKOJiIioVgwzNqpHWMXp2cev5kNfbpC5GiIiItvFMGOjIlq4wMNJBV2ZAWfTC+Quh4iIyGYxzNgohUJCt5uHmo5yEjAREVGNGGZsWOUk4CO83gwREVGNGGZsWPfQm7c14JWAiYiIasQwY8MqDzMlXy/C9Rs6eYshIiKyUQwzNszDSYVIP1cAQAL3zhAREVWLYcbGVV5vhhfPIyIiqh7DjI2rnDfDi+cRERFVj2HGxvUI8wQAHEvJQ7lByFsMERGRDWKYsXFt/Nzg4qhEYWk5zmfx4nlERER/xTBj45QKCV05b4aIiKhGDDN2gBfPIyIiqhnDjB3owYvnERER1ahBYSYlJQVXr141vj548CBmz56Njz/+2GyF0S2VF8+7kHUD+cV6eYshIiKyMQ0KM5MnT8aOHTsAABkZGYiJicHBgwfx6quvYvHixWYtkIAWrmqEt3AGAMRfzpG5GiIiItvSoDBz8uRJREdHAwC+++47dOrUCfv27cOaNWuwevVqc9ZHN/WL9AEA7DmfLXMlREREtqVBYUav10OtVgMAtm7dijFjxgAA2rdvj/T0dPNVR0YDb4aZvRcYZoiIiG7XoDATFRWFlStXYs+ePYiLi8M999wDAEhLS0OLFi3MWiBV6NfaBwqpYt5Men6x3OUQERHZjAaFmXfeeQerVq3CkCFD8PDDD6Nr164AgB9//NF4+InMy8NZhS4tPQHwUBMREdHtHBqy0pAhQ5CdnQ2tVgsvLy/j+FNPPQVnZ2ezFUemBrbxQUJKHvacz8aDvULkLoeIiMgmNGjPTHFxMXQ6nTHIJCcnY9myZUhMTISfn59ZC6RbBrbxBQD8cSEbBt6niYiICEADw8zYsWPx5ZdfAgDy8vLQp08fvPfeexg3bhw++ugjsxVXXl6O+fPnIyIiAk5OTmjdujVef/11CNE8/yHvHuoJF0clcgpLcTpdK3c5RERENqFBYebIkSMYOHAgAGDDhg3w9/dHcnIyvvzySyxfvtxsxb3zzjv46KOP8OGHH+LMmTN45513sHTpUnzwwQdm+wx7olIq0Ld1xQRrzpshIiKq0KAwU1RUBDc3NwDAli1bMGHCBCgUCtx1111ITk42W3H79u3D2LFjMWrUKISHh2PixIkYMWIEDh48aLbPsDcDjNebuSZzJURERLahQROAIyMjsWnTJowfPx6///47XnjhBQBAVlYW3N3dzVZcv3798PHHH+PcuXNo27Ytjh07hr179+L999+vcR2dTgedTmd8rdVWHI7R6/XQ6817K4DK7Zl7u7XpG1ExT+nQ5RxoC0vg5Ki02mfLTY5+N2fst3Wx39bFfltXQ/pdn2Ul0YAJKBs2bMDkyZNRXl6Ou+++G3FxcQCAJUuWYPfu3fjtt9/qu8lqGQwGvPLKK1i6dCmUSiXKy8vx5ptvYt68eTWus3DhQixatKjK+Nq1a5vEmVZCAAuPKJFXKuHpDuXo4Nk85w8REVHTVlRUhMmTJyM/P/+OO0oaFGaAinsypaeno2vXrlAoKo5WHTx4EO7u7mjfvn1DNlnFt99+i7lz5+Ldd99FVFQUEhISMHv2bLz//vuYMmVKtetUt2cmJCQE2dnZZt1rBFSkxri4OMTExEClUpl127WZt/EUNhxJxfR+YZg3sp3VPlducvW7uWK/rYv9ti7227oa0m+tVgsfH586hZkGHWYCgICAAAQEBBjvnt2yZUuzXzBv7ty5ePnll/HQQw8BADp37ozk5GQsWbKkxjCjVquNt1q4nUqlstgvrCW3XZ3B7fyw4Ugq9l3KaZZ/CK3d7+aO/bYu9tu62G/rqk+/6/NzadAEYIPBgMWLF8PDwwNhYWEICwuDp6cnXn/9dRgMhoZsslpFRUXGvT6VlEqlWT/DHvWP9IEkAWczCpClLZG7HCIiIlk1aM/Mq6++ik8//RRvv/02+vfvDwDYu3cvFi5ciJKSErz55ptmKW706NF48803ERoaiqioKBw9ehTvv/8+pk+fbpbt2ytvF0d0CvLAidR87L2QjQk9WspdEhERkWwaFGa++OILfPLJJ8a7ZQNAly5dEBwcjGeffdZsYeaDDz7A/Pnz8eyzzyIrKwtBQUH4+9//jgULFphl+/ZsQBsfnEjNx57zDDNERNS8NSjM5OTkVDvJt3379sjJyWl0UZXc3NywbNkyLFu2zGzbbCoGtvHBRzsvYu+FbAghIEmS3CURERHJokFzZrp27YoPP/ywyviHH36ILl26NLoourOeYV5wUilxrUCHxMwCucshIiKSTYP2zCxduhSjRo3C1q1b0bdvXwDA/v37kZKSgl9//dWsBVL11A5KREd4Y9e5a9hzLhvtA8x72jkREZG9aNCemcGDB+PcuXMYP3488vLykJeXhwkTJuDUqVP46quvzF0j1WBgm5u3NrjA+zQREVHz1eDrzAQFBVWZ6Hvs2DF8+umn+PjjjxtdGN3ZwDa+AM7gz0vXUaIvh0bVfG5tQEREVKlBe2bINrT1d4Wfmxq6MgMOJ+fKXQ4REZEsGGbsmCRJGHDzUNNu3kWbiIiaKYYZO1c5b2bvec6bISKi5qlec2YmTJhQ6/t5eXmNqYUaoH9kRZg5labF9Rs6tHCtel8qIiKipqxeYcbDw+OO7z/++OONKojqx89Ng/YBbjibUYC9F7Ixtluw3CURERFZVb3CzOeff26pOqgRBrX1rQgz5xlmiIio+eGcmSZgwM1DTXvOV9zagIiIqDlhmGkCoiO84eigQIa2BBev3ZC7HCIiIqtimGkCNColosO9AQC7z/GsJiIial4YZpqIyuvN7OWtDYiIqJlhmGkiKq83c+DSdZSWGWSuhoiIyHoYZpqIDgHuaOHiiKLSchy5wlsbEBFR88Ew00QoFJJx78zW05kyV0NERGQ9DDNNyD2dAgEAv55I5ynaRETUbDDMNCFD2vnCxVGJtPwSHE3Jk7scIiIiq2CYaUI0KiWGd/QHAPxyPF3maoiIiKyDYaaJubdzxaGm306kw2DgoSYiImr6GGaamMFteaiJiIiaF4aZJkajUiKGh5qIiKgZYZhpgkZ1CQJQcVYTDzUREVFTxzDTBA1s4wNXtQMytCU4msIL6BERUdPGMNME3X6o6WceaiIioiaOYaaJGtX51gX0eKiJiIiaMoaZJmpgWx+4qR2QqdXhMO/VRERETRjDTBOlduBZTURE1DwwzDRho7rwUBMRETV9DDNN2IA2PnDTOCCrQIf4ZB5qIiKipsnmw0xqaioeffRRtGjRAk5OTujcuTPi4+PlLssuqB2UGNExAADwy/E0mashIiKyDJsOM7m5uejfvz9UKhV+++03nD59Gu+99x68vLzkLs1ujOpSEWZ+O5mBch5qIiKiJshB7gJq88477yAkJASff/65cSwiIkLGiuzPgEjfW4eaLuegT6sWcpdERERkVjYdZn788UfExsbigQcewK5duxAcHIxnn30WTz75ZI3r6HQ66HQ642utVgsA0Ov10Ov1Zq2vcnvm3q45SQBiOvjhh6Np+OlYKnqEuMtdUoPZQ7+bEvbbuthv62K/rash/a7PspIQwmaPPWg0GgDAnDlz8MADD+DQoUOYNWsWVq5ciSlTplS7zsKFC7Fo0aIq42vXroWzs7NF67VVp3MlrDqrhLtKYFHPcigkuSsiIiKqXVFRESZPnoz8/Hy4u9f+H3GbDjOOjo7o1asX9u3bZxx7/vnncejQIezfv7/adarbMxMSEoLs7Ow7NqO+9Ho94uLiEBMTA5VKZdZtm1NpmQF939kJbUkZvp7eC30ivOUuqUHspd9NBfttXey3dbHf1tWQfmu1Wvj4+NQpzNj0YabAwEB07NjRZKxDhw74/vvva1xHrVZDrVZXGVepVBb7hbXkts1BpQJiowKw/vBV/H76Gga09Ze7pEax9X43Ney3dbHf1sV+W1d9+l2fn4tNn83Uv39/JCYmmoydO3cOYWFhMlVkvyovoMezmoiIqKmx6TDzwgsv4MCBA3jrrbdw4cIFrF27Fh9//DFmzJghd2l2p3+kDzycVMi+ocPBpBy5yyEiIjIbmw4zvXv3xsaNG/HNN9+gU6dOeP3117Fs2TI88sgjcpdmd1RKBe6JunkBvRO8gB4RETUdNh1mAOC+++7DiRMnUFJSgjNnztR6WjbV7t6bh5o2n8xAWblB5mqIiIjMw+bDDJlPv9Yt4OmsQvaNUh5qIiKiJoNhphm5/VDTj8d4qImIiJoGhplmZnz3YADA/xLSoC3hlS+JiMj+Mcw0M9ER3mjr74pifTl+OHxV7nKIiIgajWGmmZEkCY/eVXGdnq//vAIbvgA0ERFRnTDMNEPjuwfD2VGJC1k3cOASJwITEZF9Y5hphtw0KuPcma8PJMtcDRERUeMwzDRTlYeafj+VgUxticzVEBERNRzDTDPVIdAdvcO9UGYQ+PZgitzlEBERNRjDTDNWuXdm7cFk6HlFYCIislMMM83YPZ0C0MLFEZlaHbadyZS7HCIiogZhmGnG1A5KTOodAgD4+sAVmashIiJqGIaZZm5yn1BIErD3QjYuXrshdzlERET1xjDTzLX0csaw9n4AgDXcO0NERHaIYYaME4HXH05BUWmZzNUQERHVD8MMYVAbX4R6O6OgpAw/8W7aRERkZxhmCAqFhEf6hAIAvjqQzPs1ERGRXWGYIQDAA71C4OigwMlULY5dzZe7HCIiojpjmCEAgLeLI+7rEggA+Go/79dERET2g2GGjB67ORH4p+NpyC0slbkaIiKiumGYIaNuIZ6ICnJHaZkB6w/zfk1ERGQfGGbISJIk496ZNX9egcHAicBERGT7GGbIxJhuQXDTOCD5ehH2XMiWuxwiIqI7YpghE86ODpjYsyUA4It9l+UthoiIqA4YZqiKx+4Kg0ICtp/Nwgmepk1ERDaOYYaqaOXrirHdggEA/9p6TuZqiIiIascwQ9V6flgbKBUStp/NwpEruXKXQ0REVCOGGapWhI8LJnS/uXcmjntniIjIdjHMUI2eH9YGDgoJe85n42BSjtzlEBERVYthhmoU4u2MB3uHAADej0uUuRoiIqLqMcxQrWYOjYSjUoEDl3Kw7yKvO0NERLbHrsLM22+/DUmSMHv2bLlLaTaCPJ3wcPTNvTNbzkEIXhWYiIhsi92EmUOHDmHVqlXo0qWL3KU0O88OjYTaQYH45FzsPs+9M0REZFvsIszcuHEDjzzyCP773//Cy8tL7nKaHX93DR69ec+m9+O4d4aIiGyLg9wF1MWMGTMwatQoDB8+HG+88Uaty+p0Ouh0OuNrrVYLANDr9dDr9Watq3J75t6uLfpb/1Cs/TMZx1LysOVUOu5u52v1GppTv20B+21d7Ld1sd/W1ZB+12dZmw8z3377LY4cOYJDhw7VafklS5Zg0aJFVca3bNkCZ2dnc5cHAIiLi7PIdm1NP18FtqUp8PrGIyjuXA5JkqeO5tJvW8F+Wxf7bV3st3XVp99FRUV1Xtamw0xKSgpmzZqFuLg4aDSaOq0zb948zJkzx/haq9UiJCQEI0aMgLu7u1nr0+v1iIuLQ0xMDFQqlVm3bYvuKizF3e/vwdXCcqgiemJER3+rfn5z67fc2G/rYr+ti/22rob0u/LISl3YdJg5fPgwsrKy0KNHD+NYeXk5du/ejQ8//BA6nQ5KpdJkHbVaDbVaXWVbKpXKYr+wlty2LfH3VGH6gAh8sP0Clm+/hJGdg6FQWH/3THPpt61gv62L/bYu9tu66tPv+vxcbHoC8LBhw3DixAkkJCQYH7169cIjjzyChISEKkGGLO9vA1rBTeOAxMwC/HoyXe5yiIiIbHvPjJubGzp16mQy5uLighYtWlQZJ+vwcFbhbwNa4V9bz2HZ1vMY2SkQShn2zhAREVWy6T0zZJumDwiHh5MKF7Ju4MdjqXKXQ0REzZzdhZmdO3di2bJlcpfRrLlpVHhqUCsAwLKt56ErK5e5IiIias7sLsyQbZjaLxw+rmokXy/Cyp2X5C6HiIiaMYYZahAXtQNeG90RALBixwVcvHZD5oqIiKi5YpihBruvSyCGtPNFabkBr/xwgrc5ICIiWTDMUINJkoTXx3aCk0qJP5NysD7+qtwlERFRM8QwQ40S4u2MOTFtAQBv/noG2Td0d1iDiIjIvBhmqNGm9Q9HVJA78ov1eOPn03KXQ0REzQzDDDWag1KBJRM6QyEBmxLSsOvcNblLIiKiZoRhhsyiS0tPTOkXDgD456YTKC7ltWeIiMg6GGbIbF4c0Q5BHhqk5BTj39vOy10OERE1EwwzZDauagcsHltxz6z/7rmE02l1v307ERFRQzHMkFkN7+iPkZ0CUG4QmLfxBMoNvPYMERFZFsMMmd3CMVFwUzvgWEoevj6QLHc5RETUxDHMkNn5u2vw0j3tAABLN59Fen6xzBUREVFTxjBDFvFInzD0CPVEYWk5XvvfKbnLISKiJoxhhixCoZCwZEIXOCgkbDmdie/iU+QuiYiImiiGGbKYdgFumD28DQBg/qaTOJmaL3NFRETUFDHMkEU9OyQSd7f3g67MgGfWHEZ+kV7ukoiIqIlhmCGLUigk/OvBbgjxdkJKTjFmrzsKA0/XJiIiM2KYIYvzcFbho0d6Qu2gwI7Ea/hwxwW5SyIioiaEYYasolOwB94YV3F14H9tPYediVkyV0RERE0FwwxZzQO9QvBwdCiEAGavS0BKTpHcJRERURPAMENW9drojujS0gN5RXo8u+YISvS8uzYRETUOwwxZlUalxH8e6QFPZxVOpOZj0U+8oB4RETUOwwxZXUsvZyx/qDskCfjmYAq+O8QL6hERUcMxzJAsBrX1xQvD2wIA/vk/XlCPiIgajmGGZDNzaMUF9UrLDHj668PILSyVuyQiIrJDDDMkm9svqHc1txhTPz+IghJeIZiIiOqHYYZk5eGswqdTesPLWYVjV/PxxOp4FJWWyV0WERHZEYYZkl1bfzd89UQfuGkccPByDp768jBP2SYiojpjmCGb0CnYA6unRcPZUYm9F7Lx7JojKC0zyF0WERHZAYYZshk9w7zwyZReUDsosP1sFmavO4qycgYaIiKqnc2HmSVLlqB3795wc3ODn58fxo0bh8TERLnLIgvp19oHqx7rCZVSwq8nMvDShuO8yzYREdXK5sPMrl27MGPGDBw4cABxcXHQ6/UYMWIECgsL5S6NLGRIOz98OLkHlAoJPxxNxaubTkIIBhoiIqqeg9wF3MnmzZtNXq9evRp+fn44fPgwBg0aJFNVZGmxUQH416RumPXtUXxz8Ao0KgXmxbaRuywiIrJBNh9m/io/v+JKsd7e3tW+r9PpoNPpjK+1Wi0AQK/XQ6837zVMKrdn7u1ShZEdfVE4LgrzNp7C539chkoSiAL7bS38/bYu9tu62G/raki/67OsJOxo/73BYMCYMWOQl5eHvXv3VrvMwoULsWjRoirja9euhbOzs6VLJAvYkyFhQ5ISADAsyID7Qg1QSDIXRUREFlVUVITJkycjPz8f7u7utS5rV2HmmWeewW+//Ya9e/eiZcuW1S5T3Z6ZkJAQZGdn37EZ9aXX6xEXF4eYmBioVCqzbptMffrHZby9+RwAYEQHX/zfxC5wclTKXFXTxt9v62K/rYv9tq6G9Fur1cLHx6dOYcZuDjPNnDkTP//8M3bv3l1jkAEAtVoNtVpdZVylUlnsF9aS26YKTw9pA29nR8zbeAJbzlzDo5/H47+P94K/u0bu0po8/n5bF/ttXey3ddWn3/X5udj82UxCCMycORMbN27E9u3bERERIXdJJJPx3YMwo2M5vJxVOH41H2M//IN32yYiItsPMzNmzMDXX3+NtWvXws3NDRkZGcjIyEBxcbHcpZEMWrsD6//eB619XZChLcEDK/fj91MZcpdFREQysvkw89FHHyE/Px9DhgxBYGCg8bFu3Tq5SyOZhHk744dn+2NgGx8U68vx9NeHsXLXRV6LhoiombL5MCOEqPYxdepUuUsjGXk4qfD51N549K5QCAG8/dtZvLThOO/nRETUDNl8mCGqiYNSgdfHdsLC0R2hkID1h6/isU//RG5hqdylERGRFTHMkF2TJAlT+0fg06m94ap2wJ9JObh3+R7sPZ8td2lERGQlDDPUJAxt54fvn+mHsBbOSM8vwaOf/onX/ncSxaXlcpdGREQWxjBDTUa7ADf8+vxAPHpXKADgi/3JuHf5HhxOzpW5MiIisiSGGWpSXNQOeGNcZ3w5PRoB7hokZRfigZX78M7ms9CVcS8NEVFTxDBDTdKgtr74ffYgTOgeDIMAPtp5EWM//AOn07Ryl0ZERGbGMENNloezCu9P6oaVj/aAt4sjzmYUYOyKvVix4wLKynkKNxFRU8EwQ03ePZ0C8fvsQYjp6A99ucC7vydiwkf7cOhyjtylERGRGTDMULPg66bGx4/1xHsPdIWb2gHHr+bjgZX78fev4nHp2g25yyMiokZgmKFmQ5Ik3N+zJbb9v8F4ODoUCgn4/VQmRvxrNxb+eAo5vNgeEZFdYpihZsfPTYMlEzpj8+xBGNrOF2UGgdX7LmPw0h34aOdFlOh51hMRkT1hmKFmq62/Gz6fFo01f+uDjoHuKNCV4Z3NZ3H3/+3ExqNXYTDwxpVERPaAYYaavf6RPvj5uQF474GuCPTQIC2/BC+sO4bRH+7FT8fSeOYTEZGNY5ghAqBQVMyn2fH/hmBubDu4qh1wKk2L5745isHv7sQney6hoEQvd5lERFQNhhmi22hUSswYGoldc4dg1rA28HZxRGpeMd745Qz6LdmON385jdS8YrnLJCKi2zDMEFWjhasaL8S0xb6X78aSCZ3R2tcFBboy/HdPEgYt3YHnvzmK41fz5C6TiIgAOMhdAJEt06iUeDg6FJN6hWDnuSx8sicJ+y5ex4/H0vDjsTRER3hjUq8Q3NMpAC5q/nEiIpID//YlqgOFQsLd7f1xd3t/nEzNx6d7k/DTsTQcTMrBwaQc/HPTScRG+WN8j5YYEOkDpUKSu2QiomaDYYaonjoFe+Bfk7rhH/e0x7pDKdh49CouXy/CpoQ0bEpIg6+bGmO7BmF8j2B0DHSHJDHYEBFZEsMMUQMFeGgwa3gbPD8sEkdT8rDpaCp+OpaGawU6fLI3CZ/sTUI7fzeM7xGMkZ0CENbCRe6SiYiaJIYZokaSJAk9Qr3QI9QL/xzVETsTs7DxaCq2nclCYmYB3v7tLN7+7Sxa+brg7nZ+uLu9H3qFe8PRgfPviYjMgWGGyIwcHRQYERWAEVEByC/S49eT6fgxIQ2HLufg0rVCXLpWscfGTe2AgW19MLSdH4a084Ovm1ru0omI7BbDDJGFeDir8HB0KB6ODoW2RI8957Kx/WwWdiZm4XphKX49kYFfT2RAkoAuLT0xMNIHvSO80TPMC648M4qIqM74NyaRFbhrVBjVJRCjugTCYBA4djUPO85mYXtiFk6manEsJQ/HUvKAHYBCAjoGuaN3uDeiw73RO8IbPq7cc0NEVBOGGSIrUygkdA/1QvdQL8wZ0Q6Z2hLsTMzCn0k5OHQ5Byk5xTiZqsXJVC0+/+MyAKCVjwt6h1fstYkKdkcbPzfOuSEiuolhhkhm/u4aTOodikm9QwEA6fnFOHQ5F4duhpvEzAJcyi7EpexCrItPAQA4KhVoG+CKqEAPRAW7IyrIAx0C3eDsyD/SRNT88G8+IhsT6OGEMV2dMKZrEAAgv0iP+OQcHLycg+Mp+TiVlg9tSZlx7w3iK9ZTSECEjws6Bnkg0tcVrf1c0NrXFRE+LtColDJ+R0RElsUwQ2TjPJxVGNbBH8M6+AMAhBC4mluMU2n5OJmqxam0fJxK0yKrQIeL1wpx8VqhyfqSBLT0ckJrX1fjo5WvC0K9neHvruHVionI7jHMENkZSZIQ4u2MEG9n3NMp0DieVVCCU2lanE0vwKVrN3Dx2g1cyLoBbUkZUnKKkZJTjJ2J10y2pVJKCPJ0QksvJ4R4OVd89a74GuDmCIOw9ndHRFR/DDNETYSfmwZ+7TQY2s7POCaEwPXCUlzMunFzr01FyEnKLkRqbjH05QLJ14uQfL0IwPUq21RAiXdO74K/hxP83dTwd9cgwEMDv5vP/d018HVTw9NJBQX38BCRTBhmiJowSZLg46qGj6safVq1MHmv3CCQqS3B1dxipOQUVXzNLcLV3Irn6fklKDcAGVodMrS6Wj9HIQFezo7wdql4+Liqjc9buDrCy9kRHk4qeDip4Olc8dVNo+IhLiIyC7sIMytWrMC7776LjIwMdO3aFR988AGio6PlLovIrikVFYeYgjydEB3hXeX94hId1v+0GZ1690d2YRkyC3TI0pYgI7/k1nNtCfKK9DAI4HphKa4XltarBjeNgzHguGtUcFU7wFXjALebX13Vqluv1Q5wUTvARa2Es6MSTo4OcHFUwslRCUelgjf0JGrGbD7MrFu3DnPmzMHKlSvRp08fLFu2DLGxsUhMTISfn9+dN0BEDeKgVMDDEegc7AGVSlXjcvpyA3JvBpmcm1+v39CZPM8r0iO/+NajqLQcAFBQUoaCkjJczS1uVK1KhQRnR+XNhwM0KiU0KgU0Dje/qpS3xm4+Vzso4OiggNpBWfFVqYBapYCj8WvFuEopQaVU3HyuMI45KiteOyglqBQKHmYjkpHNh5n3338fTz75JKZNmwYAWLlyJX755Rd89tlnePnll2WujohUSgX83DXwc9fUeZ3SMgO0JbcFnCI9tCV63NCV4UZJGW7oKkKOyWtdGQpK9CguLUdRaTmKS8tRWm4AUHHIrDIYAbUfErMUhVQRAB0UEhwUkjHoOCgqv1Y8VyokOCiliq+Kyq8KKCSB7GsK/Jh7FA7KiuUUUsX7SoUE5c3nCoUEhQQopcrnknFZhVQR7CTJdBlJgvF9hXTr/YrnML6WUMP4zfcqxyTAuM3K54BUZeyv61Us9Zfxm4MVa8E4ZlxegslnwPj8L8vd/Dnces902dtJElBWVobUQuBMegFUKgeT92paX/rLNv6y1SqfUfsSt2qvbZnqtiVVu1Tt6zRGXbblplbBw7nm//RYmk2HmdLSUhw+fBjz5s0zjikUCgwfPhz79++vdh2dTged7tZfZlqtFgCg1+uh1+vNWl/l9sy9Xaoe+21dluy3BMBDrYCHWg14NvxWDfpyQ0W40ZcbQ05RaTlKysqh0xtQrC9Hid4AXVnF1xJ9OXRlt77qygwoLTOgtLziub5yrNwAnb7ia1m5AfpygdKbX/XlFePiL2d6GURFSKvfgba/UuB03rU7L0Zm4oClx6v/t4Tq5+lBEXgxpk2N7zfk75P6LGvTYSY7Oxvl5eXw9/c3Gff398fZs2erXWfJkiVYtGhRlfEtW7bA2dnZInXGxcVZZLtUPfbbuuy530oALjcfVTigUX8DGgRQZgDKhenDUPncYPraIIBySDDcfH77o/J9A26NCfxlOQDC+J5ksoy47X3jWHWv/zKG216Lmp7/ZQzG5xX/XTfg1nYq37t9u/jLusbXN5/cnglvX7e6ZW9f5q/Pq6vBZLka3qt1m6KG8VrWr+s6d9xOTWN12Kg5r6hQ120lXbyIX/Xn77hcff4+KSoqqvOyNh1mGmLevHmYM2eO8bVWq0VISAhGjBgBd3d3s36WXq9HXFwcYmJiap1TQObBflsX+21d7Ld1sd/W1ZB+Vx5ZqQubDjM+Pj5QKpXIzMw0Gc/MzERAQEC166jVaqjVVXdbq1Qqi/3CWnLbVBX7bV3st3Wx39bFfltXffpdn5+LTd9219HRET179sS2bduMYwaDAdu2bUPfvn1lrIyIiIhshU3vmQGAOXPmYMqUKejVqxeio6OxbNkyFBYWGs9uIiIioubN5sPMpEmTcO3aNSxYsAAZGRno1q0bNm/eXGVSMBERETVPNh9mAGDmzJmYOXOm3GUQERGRDbLpOTNEREREd8IwQ0RERHaNYYaIiIjsGsMMERER2TWGGSIiIrJrDDNERERk1xhmiIiIyK4xzBAREZFdY5ghIiIiu2YXVwBuDCEEgPrdSryu9Ho9ioqKoNVqeddVK2C/rYv9ti7227rYb+tqSL8r/92u/He8Nk0+zBQUFAAAQkJCZK6EiIiI6qugoAAeHh61LiOJukQeO2YwGJCWlgY3NzdIkmTWbWu1WoSEhCAlJQXu7u5m3TZVxX5bF/ttXey3dbHf1tWQfgshUFBQgKCgICgUtc+KafJ7ZhQKBVq2bGnRz3B3d+cfBitiv62L/bYu9tu62G/rqm+/77RHphInABMREZFdY5ghIiIiu8Yw0whqtRqvvfYa1Gq13KU0C+y3dbHf1sV+Wxf7bV2W7neTnwBMRERETRv3zBAREZFdY5ghIiIiu8YwQ0RERHaNYYaIiIjsGsNMA61YsQLh4eHQaDTo06cPDh48KHdJTcLu3bsxevRoBAUFQZIkbNq0yeR9IQQWLFiAwMBAODk5Yfjw4Th//rw8xTYBS5YsQe/eveHm5gY/Pz+MGzcOiYmJJsuUlJRgxowZaNGiBVxdXXH//fcjMzNTport20cffYQuXboYLxzWt29f/Pbbb8b32WvLevvttyFJEmbPnm0cY8/NZ+HChZAkyeTRvn174/uW7DXDTAOsW7cOc+bMwWuvvYYjR46ga9euiI2NRVZWltyl2b3CwkJ07doVK1asqPb9pUuXYvny5Vi5ciX+/PNPuLi4IDY2FiUlJVautGnYtWsXZsyYgQMHDiAuLg56vR4jRoxAYWGhcZkXXngBP/30E9avX49du3YhLS0NEyZMkLFq+9WyZUu8/fbbOHz4MOLj43H33Xdj7NixOHXqFAD22pIOHTqEVatWoUuXLibj7Ll5RUVFIT093fjYu3ev8T2L9lpQvUVHR4sZM2YYX5eXl4ugoCCxZMkSGatqegCIjRs3Gl8bDAYREBAg3n33XeNYXl6eUKvV4ptvvpGhwqYnKytLABC7du0SQlT0V6VSifXr1xuXOXPmjAAg9u/fL1eZTYqXl5f45JNP2GsLKigoEG3atBFxcXFi8ODBYtasWUII/n6b22uvvSa6du1a7XuW7jX3zNRTaWkpDh8+jOHDhxvHFAoFhg8fjv3798tYWdOXlJSEjIwMk957eHigT58+7L2Z5OfnAwC8vb0BAIcPH4Zerzfpefv27REaGsqeN1J5eTm+/fZbFBYWom/fvuy1Bc2YMQOjRo0y6S3A329LOH/+PIKCgtCqVSs88sgjuHLlCgDL97rJ32jS3LKzs1FeXg5/f3+TcX9/f5w9e1amqpqHjIwMAKi295XvUcMZDAbMnj0b/fv3R6dOnQBU9NzR0RGenp4my7LnDXfixAn07dsXJSUlcHV1xcaNG9GxY0ckJCSw1xbw7bff4siRIzh06FCV9/j7bV59+vTB6tWr0a5dO6Snp2PRokUYOHAgTp48afFeM8wQEYCK/72ePHnS5Bg3mV+7du2QkJCA/Px8bNiwAVOmTMGuXbvkLqtJSklJwaxZsxAXFweNRiN3OU3eyJEjjc+7dOmCPn36ICwsDN999x2cnJws+tk8zFRPPj4+UCqVVWZgZ2ZmIiAgQKaqmofK/rL35jdz5kz8/PPP2LFjB1q2bGkcDwgIQGlpKfLy8kyWZ88bztHREZGRkejZsyeWLFmCrl274t///jd7bQGHDx9GVlYWevToAQcHBzg4OGDXrl1Yvnw5HBwc4O/vz55bkKenJ9q2bYsLFy5Y/PebYaaeHB0d0bNnT2zbts04ZjAYsG3bNvTt21fGypq+iIgIBAQEmPReq9Xizz//ZO8bSAiBmTNnYuPGjdi+fTsiIiJM3u/ZsydUKpVJzxMTE3HlyhX23EwMBgN0Oh17bQHDhg3DiRMnkJCQYHz06tULjzzyiPE5e245N27cwMWLFxEYGGj53+9GTyFuhr799luhVqvF6tWrxenTp8VTTz0lPD09RUZGhtyl2b2CggJx9OhRcfToUQFAvP/+++Lo0aMiOTlZCCHE22+/LTw9PcX//vc/cfz4cTF27FgREREhiouLZa7cPj3zzDPCw8ND7Ny5U6SnpxsfRUVFxmWefvppERoaKrZv3y7i4+NF3759Rd++fWWs2n69/PLLYteuXSIpKUkcP35cvPzyy0KSJLFlyxYhBHttDbefzSQEe25OL774oti5c6dISkoSf/zxhxg+fLjw8fERWVlZQgjL9pphpoE++OADERoaKhwdHUV0dLQ4cOCA3CU1CTt27BAAqjymTJkihKg4PXv+/PnC399fqNVqMWzYMJGYmChv0Xasul4DEJ9//rlxmeLiYvHss88KLy8v4ezsLMaPHy/S09PlK9qOTZ8+XYSFhQlHR0fh6+srhg0bZgwyQrDX1vDXMMOem8+kSZNEYGCgcHR0FMHBwWLSpEniwoULxvct2WtJCCEav3+HiIiISB6cM0NERER2jWGGiIiI7BrDDBEREdk1hhkiIiKyawwzREREZNcYZoiIiMiuMcwQERGRXWOYISKbcf36dfj5+eHy5csAgJ07d0KSJOP9XDZv3oxu3brBYDDIV2Qj/PX7ISLzYJghaqauXbuGZ555BqGhoVCr1QgICEBsbCz++OMP4zKSJGHTpk1Wq+nNN9/E2LFjER4eDgDo168f0tPT4eHhAQC45557oFKpsGbNmnpvOzw8HJIkVXm8/fbb5vwWiEgGDnIXQETyuP/++1FaWoovvvgCrVq1QmZmJrZt24br16/LUk9RURE+/fRT/P7778YxR0fHKnfUnTp1KpYvX47HHnus3p+xePFiPPnkkyZjbm5uDSuYiGwG98wQNUN5eXnYs2cP3nnnHQwdOhRhYWGIjo7GvHnzMGbMGAAw7h0ZP348JEkyvgaA//3vf+jRowc0Gg1atWqFRYsWoayszPi+JEn46KOPMHLkSDg5OaFVq1bYsGFDrTX9+uuvUKvVuOuuu4xj1R2WGT16NOLj43Hx4sV6f99ubm4ICAgwebi4uJh81i+//IIuXbpAo9HgrrvuwsmTJ0228f333yMqKgpqtRrh4eF47733TN7X6XT4xz/+gZCQEKjVakRGRuLTTz81Webw4cPo1asXnJ2d0a9fPyQmJtb7eyGiWxhmiJohV1dXuLq6YtOmTdDpdNUuc+jQIQDA559/jvT0dOPrPXv24PHHH8esWbNw+vRprFq1CqtXr8abb75psv78+fNx//3349ixY3jkkUfw0EMP4cyZMzXWtGfPHvTs2fOOtYeGhsLf3x979uwxjk2dOhVDhgy547p1MXfuXLz33ns4dOgQfH19MXr0aOj1egAVIeTBBx/EQw89hBMnTmDhwoWYP38+Vq9ebVz/8ccfxzfffIPly5fjzJkzWLVqFVxdXU0+49VXX8V7772H+Ph4ODg4YPr06WapnajZMsvtKonI7mzYsEF4eXkJjUYj+vXrJ+bNmyeOHTtmsgwAsXHjRpOxYcOGibfeestk7KuvvhKBgYEm6z399NMmy/Tp00c888wzNdYzduxYMX36dJOxyruo5+bmmox3795dLFy40Pj65ZdfFo899liN2xZCGO9W7eLiYvLYvXu3yWd9++23xnWuX78unJycxLp164QQQkyePFnExMSYbHfu3LmiY8eOQgghEhMTBQARFxdXbQ2Vn7F161bj2C+//CIAiOLi4lrrJ6Kacc8MUTN1//33Iy0tDT/++CPuuece7Ny5Ez169DDZy1CdY8eOYfHixca9O66urnjyySeRnp6OoqIi43J9+/Y1Wa9v37617pkpLi6GRqOpU+1OTk4mn7VkyRJ8+eWXd1xv7ty5SEhIMHn06tWrSp2VvL290a5dO2PdZ86cQf/+/U2W79+/P86fP4/y8nIkJCRAqVRi8ODBtdbRpUsX4/PAwEAAQFZW1h3rJ6LqcQIwUTOm0WgQExODmJgYzJ8/H3/729/w2muvYerUqTWuc+PGDSxatAgTJkyodnsN5ePjg9zc3Dotm5OTA19f3wZ9RmRkZL3XqysnJ6c6LadSqYzPJUkCALs93ZzIFnDPDBEZdezYEYWFhcbXKpUK5eXlJsv06NEDiYmJiIyMrPJQKG79lXLgwAGT9Q4cOIAOHTrU+Nndu3fH6dOn71hjSUkJLl68iO7du9f126qX2+vOzc3FuXPnjHV36NDB5NR1APjjjz/Qtm1bKJVKdO7cGQaDAbt27bJIbURUPe6ZIWqGrl+/jgceeADTp09Hly5d4Obmhvj4eCxduhRjx441LhceHo5t27ahf//+UKvV8PLywoIFC3DfffchNDQUEydOhEKhwLFjx3Dy5Em88cYbxnXXr1+PXr16YcCAAVizZg0OHjxY5aye28XGxmLevHnIzc2Fl5dXjcsdOHAAarXa5HDQvHnzkJqaesdDTQUFBcjIyDAZc3Z2hru7u/H14sWL0aJFC/j7++PVV1+Fj48Pxo0bBwB48cUX0bt3b7z++uuYNGkS9u/fjw8//BD/+c9/jP2aMmUKpk+fjuXLl6Nr165ITk5GVlYWHnzwwVprI6JGkHvSDhFZX0lJiXj55ZdFjx49hIeHh3B2dhbt2rUT//znP0VRUZFxuR9//FFERkYKBwcHERYWZhzfvHmz6Nevn3BychLu7u4iOjpafPzxx8b3AYgVK1aImJgYoVarRXh4uHESbW2io6PFypUrja+rmwD81FNPib///e8m602ZMkUMHjy41m2HhYUJAFUelduq/KyffvpJREVFCUdHRxEdHV1lUvSGDRtEx44dhUqlEqGhoeLdd981eb+4uFi88MILIjAwUDg6OorIyEjx2Wef1fj9HD16VAAQSUlJd+wPEVVPEkIIuYIUETVNkiRh48aNxj0adfXLL79g7ty5OHnypMkhq0rZ2dlo164d4uPjERERYaZqK+zcuRNDhw5Fbm4uPD09zbptIrIsHmYiIpsxatQonD9/HqmpqQgJCany/uXLl/Gf//zH7EGGiOwbwwwR2ZTZs2fX+F6vXr2qnEpNRMTDTERERGTXeGo2ERER2TWGGSIiIrJrDDNERERk1xhmiIiIyK4xzBAREZFdY5ghIiIiu8YwQ0RERHaNYYaIiIjsGsMMERER2bX/D03Vk1UmIv55AAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모듈 4 ▲ GPT-2 유사 신경망 구조\n",
        "### 4.1. 파라미터 초기화\n"
      ],
      "metadata": {
        "id": "-I0T6rY_zA2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the parameters, to store the knowledge of the model\n",
        "\n",
        "n_layer = 1      # 트랜스포머 레이어 수\n",
        "n_embd = 16      # 임베딩 차원\n",
        "block_size = 16  # 최대 컨텍스트 길이\n",
        "n_head = 4       # 멀티헤드 어텐션의 헤드 수\n",
        "head_dim = n_embd // n_head  # 헤드당 차원 = 4\n",
        "matrix = lambda nout, nin, std=0.08: [[Value(random.gauss(0, std)) for _ in range(nin)] for _ in range(nout)]\n",
        "state_dict = {'wte': matrix(vocab_size, n_embd), 'wpe': matrix(block_size, n_embd), 'lm_head': matrix(vocab_size, n_embd)}"
      ],
      "metadata": {
        "id": "r_rpmD0c6IrM"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2. 전체 신경망 흐름 (gpt 함수)\n",
        "- Transformer Language Model Architecture\n",
        "\n",
        " 1️⃣ Input Stage\n",
        "\n",
        "$$\n",
        "\\text{Token ID} + \\text{Position ID}\n",
        "$$\n",
        "\n",
        "↓\n",
        "\n",
        "Embedding Layer\n",
        "\n",
        "$$\n",
        "\\mathbf{X} = \\text{wte}(\\text{Token}) + \\text{wpe}(\\text{Position})\n",
        "$$\n",
        "\n",
        "- **wte**: Token Embedding  \n",
        "- **wpe**: Position Embedding  \n",
        "\n",
        "↓\n",
        "\n",
        "RMS Normalization\n",
        "\n",
        "$$\n",
        "\\mathbf{X}_{norm} = \\text{RMSNorm}(\\mathbf{X})\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "2️⃣ Transformer Block (Repeated L times)\n",
        "\n",
        "For each layer:\n",
        "\n",
        "(1) RMSNorm\n",
        "\n",
        "$$\n",
        "\\mathbf{H}_1 = \\text{RMSNorm}(\\mathbf{X})\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "(2) Multi-Head Self-Attention\n",
        "\n",
        "Query, Key, Value 계산:\n",
        "\n",
        "$$\n",
        "Q = \\mathbf{H}_1 W_Q\n",
        "$$\n",
        "$$\n",
        "K = \\mathbf{H}_1 W_K\n",
        "$$\n",
        "$$\n",
        "V = \\mathbf{H}_1 W_V\n",
        "$$\n",
        "\n",
        "Scaled Dot-Product Attention:\n",
        "\n",
        "$$\n",
        "\\text{Attention}(Q,K,V) =\n",
        "\\text{Softmax}\\left(\n",
        "\\frac{QK^T}{\\sqrt{d_k}}\n",
        "\\right)V\n",
        "$$\n",
        "\n",
        "Multi-head 결합:\n",
        "\n",
        "$$\n",
        "\\text{MHA} = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h) W_O\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "(3) Residual Connection\n",
        "\n",
        "$$\n",
        "\\mathbf{X}_1 = \\mathbf{X} + \\text{MHA}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "(4) RMSNorm\n",
        "\n",
        "$$\n",
        "\\mathbf{H}_2 = \\text{RMSNorm}(\\mathbf{X}_1)\n",
        "$$\n",
        "\n",
        "---\n",
        "(5) MLP (Feed Forward Network)\n",
        "\n",
        "$$\n",
        "\\mathbf{F} = \\text{FC}_2(\\text{ReLU}(\\text{FC}_1(\\mathbf{H}_2)))\n",
        "$$\n",
        "\n",
        "---\n",
        "(6) Residual Connection\n",
        "\n",
        "$$\n",
        "\\mathbf{X}_{next} = \\mathbf{X}_1 + \\mathbf{F}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "3️⃣ Output Layer\n",
        "\n",
        "Language Modeling Head\n",
        "\n",
        "$$\n",
        "\\text{logits} = \\mathbf{X}_{final} W_{lm}\n",
        "$$\n",
        "\n",
        "↓\n",
        "\n",
        "Next Token Probability\n",
        "\n",
        "$$\n",
        "P(\\text{token}) = \\text{Softmax}(\\text{logits})\n",
        "$$\n"
      ],
      "metadata": {
        "id": "3a8TdKUQ6PYh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "토큰 ID + 위치 ID\n",
        "\n",
        "    ↓\n",
        "[wte] 토큰 임베딩 + [wpe] 위치 임베딩\n",
        "\n",
        "    ↓\n",
        "RMSNorm 정규화\n",
        "    ↓\n",
        "\n",
        "[ 트랜스포머 레이어 반복 ]\n",
        "-  ├─ RMSNorm\n",
        "-  ├─ Multi-Head Self-Attention (Q, K, V 계산 → 소프트맥스 → 가중합)\n",
        "-  ├─ 잔차 연결(Residual)\n",
        "-  ├─ RMSNorm\n",
        "-  ├─ MLP (FC1 → ReLU → FC2)\n",
        "-  └─ 잔차 연결\n",
        "\n",
        "    ↓\n",
        "[lm_head] 선형 변환 → 로짓(logits)"
      ],
      "metadata": {
        "id": "QyQxvJ4Vvhmi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3. 핵심 구성요소 설명\n",
        "**a. 임베딩 (Embedding)**\n",
        "- 토큰 의미 + 위치 정보를 동시에 인코딩합니다.\n"
      ],
      "metadata": {
        "id": "hW3IKVOd7qWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tok_emb = state_dict['wte'][token_id]  # 토큰 → 16차원 벡터\n",
        "pos_emb = state_dict['wpe'][pos_id]    # 위치 → 16차원 벡터\n",
        "x = [t + p for t, p in zip(tok_emb, pos_emb)]  # 합산"
      ],
      "metadata": {
        "id": "N2qbq4EB7zyj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "f812fee7-0f22-4ce2-e6dd-77f724e13039"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'token_id' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-60771715.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtok_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'wte'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken_id\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# 토큰 → 16차원 벡터\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpos_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'wpe'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos_id\u001b[0m\u001b[0;34m]\u001b[0m    \u001b[0;31m# 위치 → 16차원 벡터\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# 합산\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'token_id' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**b. RMSNorm (Root Mean Square Normalization)**\n",
        "- 학습을 안정화하는 정규화입니다. GPT-2의 LayerNorm을 단순화한 버전입니다.\n"
      ],
      "metadata": {
        "id": "bx2fjYVf7-wH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rmsnorm(x):\n",
        "    ms = sum(xi * xi for xi in x) / len(x)\n",
        "    scale = (ms + 1e-5) ** -0.5\n",
        "    return [xi * scale for xi in x]"
      ],
      "metadata": {
        "id": "MMfyPJ7h7z1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**c. Multi-Head Self-Attention**\n"
      ],
      "metadata": {
        "id": "o4yFP9qw8Tfv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q = linear(x, attn_wq)   # Query: \"무엇을 찾고 있나?\"\n",
        "k = linear(x, attn_wk)   # Key:   \"나는 이런 정보다\"\n",
        "v = linear(x, attn_wv)   # Value: \"실제로 전달할 내용\"\n",
        "\n",
        "# 어텐션 스코어 = Q·K / √head_dim\n",
        "# attn_weights = softmax(scores)\n",
        "# output = Σ(attn_weights × V)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "ZlhnKQEU7z4-",
        "outputId": "90469651-0338-49a6-bda8-3575ff7af735"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'attn_wq' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-392613503.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_wq\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# Query: \"무엇을 찾고 있나?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_wk\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# Key:   \"나는 이런 정보다\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_wv\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# Value: \"실제로 전달할 내용\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 어텐션 스코어 = Q·K / √head_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'attn_wq' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1."
      ],
      "metadata": {
        "id": "4hsqFYDD7qmu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-OOYZe_z63z2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OBkG4Bu0632V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oWorhkCF635S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YRCwMcMl638H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The most atomic way to train and run inference for a GPT in pure, dependency-free Python.\n",
        "This file is the complete algorithm.\n",
        "Everything else is just efficiency.\n",
        "\n",
        "@karpathy\n",
        "\"\"\"\n",
        "\n",
        "import os       # os.path.exists\n",
        "import math     # math.log, math.exp\n",
        "import random   # random.seed, random.choices, random.gauss, random.shuffle\n",
        "random.seed(42) # Let there be order among chaos\n",
        "\n",
        "# Let there be a Dataset `docs`: list[str] of documents (e.g. a list of names)\n",
        "if not os.path.exists('input.txt'):\n",
        "    import urllib.request\n",
        "    names_url = 'https://raw.githubusercontent.com/karpathy/makemore/988aa59/names.txt'\n",
        "    urllib.request.urlretrieve(names_url, 'input.txt')\n",
        "docs = [line.strip() for line in open('input.txt') if line.strip()]\n",
        "random.shuffle(docs)\n",
        "docs = docs[:3200]\n",
        "print(f\"num docs: {len(docs)}\")\n",
        "\n",
        "# Let there be a Tokenizer to translate strings to sequences of integers (\"tokens\") and back\n",
        "uchars = sorted(set(''.join(docs))) # unique characters in the dataset become token ids 0..n-1\n",
        "BOS = len(uchars) # token id for a special Beginning of Sequence (BOS) token\n",
        "vocab_size = len(uchars) + 1 # total number of unique tokens, +1 is for BOS\n",
        "print(f\"vocab size: {vocab_size}\")\n",
        "\n",
        "# Let there be Autograd to recursively apply the chain rule through a computation graph\n",
        "class Value:\n",
        "    __slots__ = ('data', 'grad', '_children', '_local_grads') # Python optimization for memory usage\n",
        "\n",
        "    def __init__(self, data, children=(), local_grads=()):\n",
        "        self.data = data                # scalar value of this node calculated during forward pass\n",
        "        self.grad = 0                   # derivative of the loss w.r.t. this node, calculated in backward pass\n",
        "        self._children = children       # children of this node in the computation graph\n",
        "        self._local_grads = local_grads # local derivative of this node w.r.t. its children\n",
        "\n",
        "    def __add__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        return Value(self.data + other.data, (self, other), (1, 1))\n",
        "\n",
        "    def __mul__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        return Value(self.data * other.data, (self, other), (other.data, self.data))\n",
        "\n",
        "    def __pow__(self, other): return Value(self.data**other, (self,), (other * self.data**(other-1),))\n",
        "    def log(self): return Value(math.log(self.data), (self,), (1/self.data,))\n",
        "    def exp(self): return Value(math.exp(self.data), (self,), (math.exp(self.data),))\n",
        "    def relu(self): return Value(max(0, self.data), (self,), (float(self.data > 0),))\n",
        "    def __neg__(self): return self * -1\n",
        "    def __radd__(self, other): return self + other\n",
        "    def __sub__(self, other): return self + (-other)\n",
        "    def __rsub__(self, other): return other + (-self)\n",
        "    def __rmul__(self, other): return self * other\n",
        "    def __truediv__(self, other): return self * other**-1\n",
        "    def __rtruediv__(self, other): return other * self**-1\n",
        "\n",
        "    def backward(self):\n",
        "        topo = []\n",
        "        visited = set()\n",
        "        def build_topo(v):\n",
        "            if v not in visited:\n",
        "                visited.add(v)\n",
        "                for child in v._children:\n",
        "                    build_topo(child)\n",
        "                topo.append(v)\n",
        "        build_topo(self)\n",
        "        self.grad = 1\n",
        "        for v in reversed(topo):\n",
        "            for child, local_grad in zip(v._children, v._local_grads):\n",
        "                child.grad += local_grad * v.grad\n",
        "\n",
        "# Initialize the parameters, to store the knowledge of the model\n",
        "n_layer = 1     # depth of the transformer neural network (number of layers)\n",
        "n_embd = 16     # width of the network (embedding dimension)\n",
        "block_size = 16 # maximum context length of the attention window (note: the longest name is 15 characters)\n",
        "n_head = 4      # number of attention heads\n",
        "head_dim = n_embd // n_head # derived dimension of each head\n",
        "matrix = lambda nout, nin, std=0.08: [[Value(random.gauss(0, std)) for _ in range(nin)] for _ in range(nout)]\n",
        "state_dict = {'wte': matrix(vocab_size, n_embd), 'wpe': matrix(block_size, n_embd), 'lm_head': matrix(vocab_size, n_embd)}\n",
        "for i in range(n_layer):\n",
        "    state_dict[f'layer{i}.attn_wq'] = matrix(n_embd, n_embd)\n",
        "    state_dict[f'layer{i}.attn_wk'] = matrix(n_embd, n_embd)\n",
        "    state_dict[f'layer{i}.attn_wv'] = matrix(n_embd, n_embd)\n",
        "    state_dict[f'layer{i}.attn_wo'] = matrix(n_embd, n_embd)\n",
        "    state_dict[f'layer{i}.mlp_fc1'] = matrix(4 * n_embd, n_embd)\n",
        "    state_dict[f'layer{i}.mlp_fc2'] = matrix(n_embd, 4 * n_embd)\n",
        "params = [p for mat in state_dict.values() for row in mat for p in row] # flatten params into a single list[Value]\n",
        "print(f\"num params: {len(params)}\")\n",
        "\n",
        "# Define the model architecture: a function mapping tokens and parameters to logits over what comes next\n",
        "# Follow GPT-2, blessed among the GPTs, with minor differences: layernorm -> rmsnorm, no biases, GeLU -> ReLU\n",
        "def linear(x, w):\n",
        "    return [sum(wi * xi for wi, xi in zip(wo, x)) for wo in w]\n",
        "\n",
        "def softmax(logits):\n",
        "    max_val = max(val.data for val in logits)\n",
        "    exps = [(val - max_val).exp() for val in logits]\n",
        "    total = sum(exps)\n",
        "    return [e / total for e in exps]\n",
        "\n",
        "def rmsnorm(x):\n",
        "    ms = sum(xi * xi for xi in x) / len(x)\n",
        "    scale = (ms + 1e-5) ** -0.5\n",
        "    return [xi * scale for xi in x]\n",
        "\n",
        "def gpt(token_id, pos_id, keys, values):\n",
        "    tok_emb = state_dict['wte'][token_id] # token embedding\n",
        "    pos_emb = state_dict['wpe'][pos_id] # position embedding\n",
        "    x = [t + p for t, p in zip(tok_emb, pos_emb)] # joint token and position embedding\n",
        "    x = rmsnorm(x) # note: not redundant due to backward pass via the residual connection\n",
        "\n",
        "    for li in range(n_layer):\n",
        "        # 1) Multi-head Attention block\n",
        "        x_residual = x\n",
        "        x = rmsnorm(x)\n",
        "        q = linear(x, state_dict[f'layer{li}.attn_wq'])\n",
        "        k = linear(x, state_dict[f'layer{li}.attn_wk'])\n",
        "        v = linear(x, state_dict[f'layer{li}.attn_wv'])\n",
        "        keys[li].append(k)\n",
        "        values[li].append(v)\n",
        "        x_attn = []\n",
        "        for h in range(n_head):\n",
        "            hs = h * head_dim\n",
        "            q_h = q[hs:hs+head_dim]\n",
        "            k_h = [ki[hs:hs+head_dim] for ki in keys[li]]\n",
        "            v_h = [vi[hs:hs+head_dim] for vi in values[li]]\n",
        "            attn_logits = [sum(q_h[j] * k_h[t][j] for j in range(head_dim)) / head_dim**0.5 for t in range(len(k_h))]\n",
        "            attn_weights = softmax(attn_logits)\n",
        "            head_out = [sum(attn_weights[t] * v_h[t][j] for t in range(len(v_h))) for j in range(head_dim)]\n",
        "            x_attn.extend(head_out)\n",
        "        x = linear(x_attn, state_dict[f'layer{li}.attn_wo'])\n",
        "        x = [a + b for a, b in zip(x, x_residual)]\n",
        "        # 2) MLP block\n",
        "        x_residual = x\n",
        "        x = rmsnorm(x)\n",
        "        x = linear(x, state_dict[f'layer{li}.mlp_fc1'])\n",
        "        x = [xi.relu() for xi in x]\n",
        "        x = linear(x, state_dict[f'layer{li}.mlp_fc2'])\n",
        "        x = [a + b for a, b in zip(x, x_residual)]\n",
        "\n",
        "    logits = linear(x, state_dict['lm_head'])\n",
        "    return logits\n",
        "\n",
        "# Let there be Adam, the blessed optimizer and its buffers\n",
        "learning_rate, beta1, beta2, eps_adam = 0.01, 0.85, 0.99, 1e-8\n",
        "m = [0.0] * len(params) # first moment buffer\n",
        "v = [0.0] * len(params) # second moment buffer\n",
        "\n",
        "# Repeat in sequence\n",
        "num_steps = 1000 # number of training steps\n",
        "for step in range(num_steps):\n",
        "\n",
        "    # Take single document, tokenize it, surround it with BOS special token on both sides\n",
        "    doc = docs[step % len(docs)]\n",
        "    tokens = [BOS] + [uchars.index(ch) for ch in doc] + [BOS]\n",
        "    n = min(block_size, len(tokens) - 1)\n",
        "\n",
        "    # Forward the token sequence through the model, building up the computation graph all the way to the loss\n",
        "    keys, values = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]\n",
        "    losses = []\n",
        "    for pos_id in range(n):\n",
        "        token_id, target_id = tokens[pos_id], tokens[pos_id + 1]\n",
        "        logits = gpt(token_id, pos_id, keys, values)\n",
        "        probs = softmax(logits)\n",
        "        loss_t = -probs[target_id].log()\n",
        "        losses.append(loss_t)\n",
        "    loss = (1 / n) * sum(losses) # final average loss over the document sequence. May yours be low.\n",
        "\n",
        "    # Backward the loss, calculating the gradients with respect to all model parameters\n",
        "    loss.backward()\n",
        "\n",
        "    # Adam optimizer update: update the model parameters based on the corresponding gradients\n",
        "    lr_t = learning_rate * (1 - step / num_steps) # linear learning rate decay\n",
        "    for i, p in enumerate(params):\n",
        "        m[i] = beta1 * m[i] + (1 - beta1) * p.grad\n",
        "        v[i] = beta2 * v[i] + (1 - beta2) * p.grad ** 2\n",
        "        m_hat = m[i] / (1 - beta1 ** (step + 1))\n",
        "        v_hat = v[i] / (1 - beta2 ** (step + 1))\n",
        "        p.data -= lr_t * m_hat / (v_hat ** 0.5 + eps_adam)\n",
        "        p.grad = 0\n",
        "\n",
        "    print(f\"step {step+1:4d} / {num_steps:4d} | loss {loss.data:.4f}\", end='\\r')\n",
        "\n",
        "# Inference: may the model babble back to us\n",
        "temperature = 0.5 # in (0, 1], control the \"creativity\" of generated text, low to high\n",
        "print(\"\\n--- inference (new, hallucinated names) ---\")\n",
        "for sample_idx in range(20):\n",
        "    keys, values = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]\n",
        "    token_id = BOS\n",
        "    sample = []\n",
        "    for pos_id in range(block_size):\n",
        "        logits = gpt(token_id, pos_id, keys, values)\n",
        "        probs = softmax([l / temperature for l in logits])\n",
        "        token_id = random.choices(range(vocab_size), weights=[p.data for p in probs])[0]\n",
        "        if token_id == BOS:\n",
        "            break\n",
        "        sample.append(uchars[token_id])\n",
        "    print(f\"sample {sample_idx+1:2d}: {''.join(sample)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KObH9M-dsTEb",
        "outputId": "d0f7c783-92ee-46ab-8580-614eaaa3637a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num docs: 3200\n",
            "vocab size: 27\n",
            "num params: 4192\n",
            "step 1000 / 1000 | loss 2.6497\n",
            "--- inference (new, hallucinated names) ---\n",
            "sample  1: kamon\n",
            "sample  2: ann\n",
            "sample  3: karai\n",
            "sample  4: jaire\n",
            "sample  5: vialan\n",
            "sample  6: karia\n",
            "sample  7: yeran\n",
            "sample  8: anna\n",
            "sample  9: areli\n",
            "sample 10: kaina\n",
            "sample 11: konna\n",
            "sample 12: keylen\n",
            "sample 13: liole\n",
            "sample 14: alerin\n",
            "sample 15: earan\n",
            "sample 16: lenne\n",
            "sample 17: kana\n",
            "sample 18: lara\n",
            "sample 19: alela\n",
            "sample 20: anton\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Andrej Karpathy의 minGPT 혹은 micrograd 스타일의 교육용 코드입니다. 직접 구현한 Value 클래스와 수동 연산들을 PyTorch의 핵심 기능인 torch.Tensor와 nn.Module 등을 사용하여 간략하게 리팩토링해 보겠습니다.\n",
        "\n",
        "PyTorch를 사용하면 수동으로 구현했던 Autograd(역전파), Layer 구성, Adam 옵티마이저를 단 몇 줄로 줄일 수 있습니다."
      ],
      "metadata": {
        "id": "QuATcocU-AP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import os, urllib.request, random\n",
        "\n",
        "# 1. 데이터 준비 (기존과 동일)\n",
        "if not os.path.exists('input.txt'):\n",
        "    names_url = 'https://raw.githubusercontent.com/karpathy/makemore/988aa59/names.txt'\n",
        "    urllib.request.urlretrieve(names_url, 'input.txt')\n",
        "docs = [line.strip() for line in open('input.txt') if line.strip()]\n",
        "random.seed(42); random.shuffle(docs)\n",
        "# docs = docs[:3200]\n",
        "\n",
        "uchars = sorted(set(''.join(docs)))\n",
        "vocab_size = len(uchars) + 1\n",
        "BOS = len(uchars)\n",
        "char_to_it = {ch: i for i, ch in enumerate(uchars)}\n",
        "it_to_char = {i: ch for i, ch in enumerate(uchars)}\n",
        "\n",
        "# 2. 하이퍼파라미터\n",
        "n_layer, n_embd, block_size, n_head = 1, 16, 16, 4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# 3. GPT 모델 정의 (PyTorch Module 활용)\n",
        "class SimpleGPT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.wte = nn.Embedding(vocab_size, n_embd)\n",
        "        self.wpe = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[\n",
        "            nn.TransformerEncoderLayer(d_model=n_embd, nhead=n_head, dim_feedforward=4*n_embd,\n",
        "                                       activation='relu', batch_first=True, norm_first=True)\n",
        "            for _ in range(n_layer)\n",
        "        ])\n",
        "        self.ln_f = nn.RMSNorm(n_embd) # PyTorch 최신 버전 기준\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, idx):\n",
        "        b, t = idx.size()\n",
        "        pos = torch.arange(0, t, device=device).unsqueeze(0)\n",
        "        x = self.wte(idx) + self.wpe(pos)\n",
        "\n",
        "        # 인과적 마스킹(Causal Mask) 적용\n",
        "        mask = torch.triu(torch.ones(t, t, device=device) * float('-inf'), diagonal=1)\n",
        "        x = self.blocks[0].self_attn(x, x, x, attn_mask=mask)[0] + x # 단순화를 위해 레이어 직접 호출\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "        return logits\n",
        "\n",
        "model = SimpleGPT().to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01, betas=(0.85, 0.99))\n",
        "\n",
        "# 4. 학습 루프\n",
        "for step in range(1000):\n",
        "    doc = docs[step % len(docs)]\n",
        "    tokens = [BOS] + [char_to_it[ch] for ch in doc] + [BOS]\n",
        "    x_idx = torch.tensor([tokens[:-1]], device=device)\n",
        "    y_idx = torch.tensor([tokens[1:]], device=device)\n",
        "\n",
        "    logits = model(x_idx)\n",
        "    loss = F.cross_entropy(logits.view(-1, vocab_size), y_idx.view(-1))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if step % 100 == 0:\n",
        "        print(f\"step {step} | loss {loss.item():.4f}\")\n",
        "\n",
        "# 5. 추론 (Inference)\n",
        "print(\"\\n--- 생성된 이름 ---\")\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for _ in range(10):\n",
        "        out = []\n",
        "        idx = torch.tensor([[BOS]], device=device)\n",
        "        for _ in range(block_size):\n",
        "            logits = model(idx)[:, -1, :] / 0.5\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_id = torch.multinomial(probs, num_samples=1)\n",
        "            if next_id.item() == BOS: break\n",
        "            out.append(it_to_char[next_id.item()])\n",
        "            idx = torch.cat((idx, next_id), dim=1)\n",
        "        print(\"\".join(out))"
      ],
      "metadata": {
        "id": "arsgkcFPzhqn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f340e4e-fc7a-479a-8ed0-5850159c7a1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0 | loss 3.5359\n",
            "step 100 | loss 2.5243\n",
            "step 200 | loss 2.2194\n",
            "step 300 | loss 2.6814\n",
            "step 400 | loss 2.5972\n",
            "step 500 | loss 2.4916\n",
            "step 600 | loss 2.2032\n",
            "step 700 | loss 2.3732\n",
            "step 800 | loss 2.2603\n",
            "step 900 | loss 2.0710\n",
            "\n",
            "--- 생성된 이름 ---\n",
            "jerel\n",
            "nariy\n",
            "canera\n",
            "solalin\n",
            "anon\n",
            "monne\n",
            "lii\n",
            "shona\n",
            "nerela\n",
            "lleen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2-YtXXfD-DEA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}