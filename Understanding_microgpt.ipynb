{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ancestor9/MicroGPT-/blob/main/Understanding_microgpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "대형언어모델(LLM)의 복잡한 구조를 요약, 단 200줄의 순수 파이썬 코드로 GPT의 핵심 원리를 구현한 프로젝트가 공개됐다.\n",
        "\n",
        "안드레이 카르파시가 선보인 ‘마이크로GPT(MicroGPT)’는 외부 라이브러리 없이도 GPT의 학습과 추론 과정을 모두 수행하는 단일 파일 프로그램이다.\n",
        "\n",
        "거대한 인프라나 복잡한 프레임워크 없이, LLM이 작동하는 알고리즘의 본질만을 가장 단순한 형태로 담아냈다는 평가를 받고 있다.\n",
        "\n",
        "카르파시 유레카랩스 CEO는 12일(현지시간) 블로그를 통해 ‘마이크로GPT’를 소개하며, 이를 하나의 “예술 작품(art project)”이라고 표현했다.\n",
        "\n",
        "그는 GPT를 구성하는 모든 핵심 요소를 단 하나의 파일에 담았다고 설명하며, “이보다 더 단순하게 줄일 수는 없다”라고 강조했다.\n",
        "\n",
        "마이크로GPT는 단 하나의 파이썬 파일로 구성된다. 이 파일 안에는\n",
        "- ▲데이터셋 처리\n",
        "- ▲토크나이저\n",
        "- ▲자동 미분(autograd) 엔진\n",
        "- ▲GPT-2와 유사한 신경망 구조\n",
        "- ▲아담(Adam) 옵티마이저\n",
        "- ▲학습 루프\n",
        "- ▲추론 루프   \n",
        "\n",
        "까지, GPT가 작동하는 전 과정이 모두 담겨 있다.\n",
        "\n",
        "### [원문기사](https://www.aitimes.com/news/articleView.html?idxno=206917)"
      ],
      "metadata": {
        "id": "0HItlJ9_s_fa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 원래 코드를 7개 모듈로 분해하여 학습을 목적으로 각 모듈의 핵심을 짧게 요약하면:\n",
        "\n",
        "1. 데이터셋 처리 — 이름 목록을 불러와 셔플. 문서 단위 학습의 기본 구조.\n",
        "2. 토크나이저 — 문자 ↔ 정수 변환. BOS 특수 토큰으로 시작/끝 표시.\n",
        "3. Autograd 엔진 — PyTorch 없이 스칼라 수준에서 역전파 구현. 계산 그래프 + 체인 룰의 정수.\n",
        "4. GPT 신경망 — 임베딩 → RMSNorm → Multi-Head Attention → MLP → 잔차 연결의 반복. GPT-2 아키텍처의 축소판.\n",
        "5. Adam 옵티마이저 — 1차/2차 모멘텀으로 파라미터별 적응적 학습률을 계산.\n",
        "6. 학습 루프 — 순전파 → 크로스엔트로피 손실 → 역전파 → Adam 업데이트의 반복.\n",
        "7. 추론 루프 — BOS를 시작으로 온도(temperature)에 따라 확률적으로 다음 토큰을 샘플링.\n",
        "\n",
        ">> 특히 모듈 3(Autograd) 이 이 코드의 교육적 핵심으로 PyTorch가 내부적으로 하는 일을 50줄로 직접 구현한 것이라, 이 부분을 확실히 이해하면 딥러닝의 역전파 원리 전체가 명확해집니다."
      ],
      "metadata": {
        "id": "3Ype3pu1vopG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모듈 1 ▲ 데이터셋 처리"
      ],
      "metadata": {
        "id": "wjDd-5sGvo_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists('input.txt'):\n",
        "    import urllib.request\n",
        "    names_url = 'https://raw.githubusercontent.com/karpathy/makemore/988aa59/names.txt'\n",
        "    urllib.request.urlretrieve(names_url, 'input.txt')\n",
        "\n",
        "docs = [line.strip() for line in open('input.txt') if line.strip()]\n",
        "random.shuffle(docs)"
      ],
      "metadata": {
        "id": "LAjcSfG_wWR1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(docs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NhiXcD8wvJm",
        "outputId": "4b5829c5-a60d-4263-8aa8-a9ec1c0f4faa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "' / '.join(docs[:15])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "rovUISDBw0Oi",
        "outputId": "e0ff45a0-ad07-441a-f519-319304890694"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'kamirah / mykelti / aylia / viaan / dayla / janell / nilynn / miral / jakori / salome / aditi / kader / jalaya / aluna / amyla'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "설명\n",
        "\n",
        "- input.txt가 없으면 인터넷에서 이름 목록(약 32,000개)을 자동 다운로드합니다.\n",
        "- 각 줄을 하나의 문서(document) 로 취급합니다. 여기서 문서 = 사람 이름 하나.\n",
        "- random.shuffle로 순서를 섞어 학습 편향을 방지합니다.\n",
        "- 핵심 개념: 언어 모델은 문서 단위로 학습합니다. 문서가 짧은 이름이든 긴 소설이든 원리는 동일합니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "Gx1AnzLJwh5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 메모리 절약을 위해 32000개 단어만 사용\n",
        "\n",
        "docs = docs[:3200]\n",
        "print(f\"Extracted {len(docs)} documents.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0XQX527xb-2",
        "outputId": "b61303ea-cc1f-4418-aa85-2462366860c7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted 3200 documents.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모듈 2 ▲ 토크나이저"
      ],
      "metadata": {
        "id": "YEE_yFrUwWVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uchars = sorted(set(''.join(docs)))   # 데이터셋에 등장한 고유 문자들\n",
        "BOS = len(uchars)                      # 특수 토큰: 시퀀스 시작/끝을 의미\n",
        "print(f\"Unique characters: {BOS}\")\n",
        "vocab_size = len(uchars) + 1           # 전체 어휘 크기\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmQSq540x0V5",
        "outputId": "2d00806b-15b6-41fc-dc54-061f4c423980"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique characters: 26\n",
            "27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "설명\n",
        "\n",
        "- 토큰화(Tokenization): 문자열 → 정수 시퀀스 변환.\n",
        "- 이 구현은 문자 단위(character-level) 토크나이저입니다. 'emma' → [4, 12, 12, 0] 형식.\n",
        "- 실제 GPT(GPT-4 등)는 BPE(Byte Pair Encoding) 방식을 사용하지만, 원리는 동일합니다.\n",
        "- BOS(Beginning of Sequence) 토큰은 문장의 시작과 끝을 알리는 특수 신호입니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "jsRNBnMCyjSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(uchars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Rj4Nep6ys91",
        "outputId": "145d6be2-52c2-4f3b-91a3-6aaf04c03f0d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 기존 설정값\n",
        "\n",
        "char_to_int = {char: i for i, char in enumerate(uchars)}\n",
        "\n",
        "\n",
        "# 실제 토큰화 함수 (Encode)\n",
        "# 이제 'emma'라는 단어를 넣으면 숫자로 짠! 하고 변하는 함수입니다.\n",
        "def encode_with_bos(text):\n",
        "    # 1. 텍스트를 숫자로 변환\n",
        "    tokens = [char_to_int[c] for c in text]\n",
        "\n",
        "    # 2. 앞뒤를 BOS(26)로 감싸기\n",
        "    # [26] + [4, 12, 12, 0] + [26] 형태가 됩니다.\n",
        "    return [BOS] + tokens + [BOS]\n",
        "\n",
        "# --- 실행 ---\n",
        "test_word = \"emma\"\n",
        "encoded = encode_with_bos(test_word)\n",
        "\n",
        "print(f\"원본: {test_word}\")\n",
        "print(f\"BOS로 감싼 토큰화 결과: {encoded}\")\n",
        "# 출력 결과: [26, 4, 12, 12, 0, 26]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFnuKrSi01wo",
        "outputId": "5ae67ea0-0c87-4b4f-fd93-ecd8e11a8a47"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "원본: emma\n",
            "BOS로 감싼 토큰화 결과: [26, 4, 12, 12, 0, 26]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "'a' → 0, 'b' → 1, ..., 'z' → 25, BOS → 26\n",
        "\n",
        "\"emma\" → [26, 4, 12, 12, 0, 26]  (BOS로 감싸기)"
      ],
      "metadata": {
        "id": "l7MqtBC2yvcy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "마이크로GPT는 거대한 웹 데이터 대신 약 3만2000개의 영어 이름을 학습 데이터로 사용한다. 각 이름을 하나의 ‘문서(document)’로 간주하고, 이 이름들에 담긴 글자 배열의 통계적 패턴을 학습해 새로운 이름을 만들어낸다.\n",
        "\n",
        "학습이 끝난 뒤에는 kamon, karia, alerin, anton처럼 실제로 존재할 법한 새로운 이름을 생성할 수 있다. 이는 GPT가 본질적으로 ‘문서를 이어 쓰는(document completion) 모델’이라는 점을 보여준다. 우리가 챗봇과 주고받는 대화도 모델의 관점에서는 하나의 긴 문서를 계속 이어가는 과정에 불과하기 때문이다.\n",
        "\n",
        "실제 상용 모델들이 BPE(Byte Pair Encoding)와 같은 서브워드 기반 토크나이저를 사용하는 것과 달리, 마이크로GPT는 훨씬 단순한 방식을 택했다. 데이터에 등장하는 알파벳 문자(a~z) 각각에 정수 ID를 부여하고, 여기에 문서의 시작을 나타내는 BOS(Beginning of Sequence) 토큰을 더해 총 27개의 어휘만 사용한다.\n",
        "\n",
        "이렇게 변환된 텍스트는 숫자의 나열(정수 시퀀스)이 되고, 신경망은 이 숫자들을 입력으로 받아 다음에 올 문자의 확률을 예측하는 방식으로 학습을 진행한다.\n"
      ],
      "metadata": {
        "id": "9k52FTGryZc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 문자를 숫자로 (Character to Index)\n",
        "char_to_int = {char: i for i, char in enumerate(uchars)}\n",
        "\n",
        "# 숫자를 문자로 (Index to Character)\n",
        "int_to_char = {i: char for i, char in enumerate(uchars)}\n",
        "\n",
        "# BOS(문장 시작/끝) 토큰 추가 (예: 가장 마지막 번호 부여)\n",
        "BOS_TOKEN = len(uchars)\n",
        "int_to_char[BOS_TOKEN] = \"[BOS]\"\n",
        "\n",
        "print(char_to_int)\n",
        "print(int_to_char)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVJhIwowzAkh",
        "outputId": "1ef9fcff-df13-481f-f244-a6704aa07b9f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 9, 'k': 10, 'l': 11, 'm': 12, 'n': 13, 'o': 14, 'p': 15, 'q': 16, 'r': 17, 's': 18, 't': 19, 'u': 20, 'v': 21, 'w': 22, 'x': 23, 'y': 24, 'z': 25}\n",
            "{0: 'a', 1: 'b', 2: 'c', 3: 'd', 4: 'e', 5: 'f', 6: 'g', 7: 'h', 8: 'i', 9: 'j', 10: 'k', 11: 'l', 12: 'm', 13: 'n', 14: 'o', 15: 'p', 16: 'q', 17: 'r', 18: 's', 19: 't', 20: 'u', 21: 'v', 22: 'w', 23: 'x', 24: 'y', 25: 'z', 26: '[BOS]'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모듈 3 ▲ 자동 미분(Autograd) 엔진\n",
        "\n",
        "- 딥러닝의 심장이라고 할 수 있는 자동 미분(Autograd) 엔진의 핵심 원리를 구현한 클래스입니다.\n",
        "- 안드레이 카파시(Andrej Karpathy)의 micrograd 구조로, 연산 그래프를 따라 미분값(Gradient)이 어떻게 흐르는지 보여주는 다음 작동방식을 따른다.\n",
        "\n",
        "> **순전파(Forward):**\n",
        "\n",
        ">> 각 연산(+, * 등)이 수행될 때 결과값(data)을 계산하고, 나중에 미분에 써먹을 자식 노드와 로컬 미분값을 미리 저장\n",
        "\n",
        "> **역전파(Backward):**\n",
        "\n",
        ">> 결과값(보통 Loss)에서부터 시작해 거꾸로 내려오며, 저장해둔 로컬 미분값들을 곱해나가며 각 변수가 결과에 얼마나 기여했는지(grad)를 계산\n",
        "\n",
        "- 이 코드는 PyTorch의 Tensor 객체가 내부적으로 작동하는 방식을 가장 직관적으로 보여주는 모델입니다."
      ],
      "metadata": {
        "id": "-na2U9jJzAta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "class Value:\n",
        "    # 메모리 최적화: 인스턴스 변수를 고정하여 관리 (속도 향상 및 메모리 절감)\n",
        "    # __slots__는 파이썬에서 메모리 사용량을 줄이고 속도를 높이기 위해 사용하는 특수 변수\n",
        "    __slots__ = ('data', 'grad', '_children', '_local_grads')\n",
        "\n",
        "    def __init__(self, data, children=(), local_grads=()):\n",
        "        self.data = data                # 순전파(Forward) 시 계산된 실제 값 (스칼라)\n",
        "        self.grad = 0                   # 역전파(Backward) 시 계산될 손실 함수에 대한 이 노드의 미분값\n",
        "        self._children = children       # 이 노드를 만든 부모 노드들 (연산 그래프 추적용)\n",
        "        self._local_grads = local_grads # 현재 연산에서 각 자식에 대한 국소 미분값 (로컬 그래디언트)\n",
        "\n",
        "    # 더하기 연산 (a + b)\n",
        "    def __add__(self, other):\n",
        "        # 숫자가 들어오면 Value 객체로 변환\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        # 결과값 계산 및 로컬 미분값 (1, 1) 저장 (x+y를 x로 미분하면 1, y로 미분하면 1)\n",
        "        return Value(self.data + other.data, (self, other), (1, 1))\n",
        "\n",
        "    # 곱하기 연산 (a * b)\n",
        "    def __mul__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        # 로컬 미분값: x*y를 x로 미분하면 y, y로 미분하면 x\n",
        "        return Value(self.data * other.data, (self, other), (other.data, self.data))\n",
        "\n",
        "    # 거듭제곱 연산 (a ** n)\n",
        "    def __pow__(self, other):\n",
        "        # 로컬 미분값: x^n을 x로 미분하면 n * x^(n-1)\n",
        "        return Value(self.data**other, (self,), (other * self.data**(other-1),))\n",
        "\n",
        "    def log(self):\n",
        "        # log(x)를 x로 미분하면 1/x\n",
        "        return Value(math.log(self.data), (self,), (1/self.data,))\n",
        "\n",
        "    def exp(self):\n",
        "        # e^x를 x로 미분하면 e^x\n",
        "        return Value(math.exp(self.data), (self,), (math.exp(self.data),))\n",
        "\n",
        "    def relu(self):\n",
        "        # ReLU 미분: x > 0 이면 1, 아니면 0\n",
        "        return Value(max(0, self.data), (self,), (float(self.data > 0),))\n",
        "\n",
        "    # --- 연산자 오버로딩 (편의 기능) ---\n",
        "    def __neg__(self): return self * -1                # -self\n",
        "    def __radd__(self, other): return self + other      # other + self\n",
        "    def __sub__(self, other): return self + (-other)   # self - other\n",
        "    def __rsub__(self, other): return other + (-self)  # other - self\n",
        "    def __rmul__(self, other): return self * other      # other * self\n",
        "    def __truediv__(self, other): return self * other**-1 # self / other\n",
        "    def __rtruediv__(self, other): return other * self**-1 # other / self\n",
        "\n",
        "    # 역전파(Backpropagation) 엔진\n",
        "    def backward(self):\n",
        "        topo = []      # 위상 정렬된 노드 리스트\n",
        "        visited = set()\n",
        "\n",
        "        # 1. 위상 정렬 (Topological Sort): 그래프를 끝에서부터 거꾸로 순회하기 위한 순서 정하기\n",
        "        def build_topo(v):\n",
        "            if v not in visited:\n",
        "                visited.add(v)\n",
        "                for child in v._children:\n",
        "                    build_topo(child)\n",
        "                topo.append(v) # 자식들을 먼저 다 방문한 후 자신을 추가\n",
        "\n",
        "        build_topo(self)\n",
        "\n",
        "        # 2. 미분 시작\n",
        "        self.grad = 1 # 출력(Loss)에 대한 자기 자신의 미분값은 항상 1\n",
        "\n",
        "        # 3. 체인 룰(Chain Rule) 적용: 정렬된 리스트를 역순으로 순회\n",
        "        for v in reversed(topo):\n",
        "            for child, local_grad in zip(v._children, v._local_grads):\n",
        "                # 자식의 미분값 = (현재 노드의 미분값) * (현재 연산의 로컬 미분값)\n",
        "                child.grad += local_grad * v.grad"
      ],
      "metadata": {
        "id": "3Rjdm6pdzAuy"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "설명\n",
        "\n",
        "- PyTorch의 autograd와 동일한 원리를 스칼라 수준에서 구현합니다.\n",
        "- 모든 연산은 계산 그래프(computation graph) 를 형성합니다.\n",
        "- backward() 호출 시 손실(loss)에서 거꾸로 각 파라미터의 기울기를 계산합니다.\n",
        "- 체인 룰: dL/dx = dL/dy × dy/dx — 이것이 딥러닝 학습의 수학적 핵심입니다.\n",
        "\n",
        "연산 예시:\n",
        "\n",
        "> z = x * y  →  dz/dx = y,  dz/dy = x\n",
        "\n",
        "> 손실 L에서: dL/dx = dL/dz × y"
      ],
      "metadata": {
        "id": "IHZPzcIBzAxM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Value 클래스를 사용하여, 간단한 선형 회귀(Linear Regression) 모델을 학습시키는 과정\n",
        "- $y = w \\cdot x + b$라는 아주 단순한 식에서,\n",
        "- 데이터 $(x=2.0, y=5.0)$가 주어졌을 때 정답에 가까워지도록 가중치 $w$와 편향 $b$를 업데이트하는 과정을 담고 있습니다."
      ],
      "metadata": {
        "id": "wdqcQHRa4cPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 초기 파라미터 설정 (w=0.5, b=0.0으로 시작)\n",
        "w = Value(0.5); b = Value(0.0)\n",
        "\n",
        "# 학습 데이터 (입력 2.0일 때 정답은 5.0이라고 가정)\n",
        "x = 2.0\n",
        "target = 5.0\n",
        "\n",
        "# 학습 횟수 (Epochs)\n",
        "for i in range(50):\n",
        "    # --- [순전파 (Forward Pass)] ---\n",
        "    # 예측값 계산: y = w * x + b\n",
        "    pred = w * x + b\n",
        "\n",
        "    # 손실 함수 계산: (예측값 - 정답)^2 (Mean Squared Error 방식)\n",
        "    loss = (pred - target)**2\n",
        "\n",
        "    # --- [역전파 (Backward Pass)] ---\n",
        "    # 기존 미분값 초기화 (매 단계 새로 계산해야 함)\n",
        "    w.grad = 0; b.grad = 0\n",
        "\n",
        "    # 전체 그래프를 거슬러 올라가며 미분값(grad) 계산\n",
        "    loss.backward()\n",
        "\n",
        "    # --- [파라미터 업데이트 (Optimizer)] ---\n",
        "    # 학습률(Learning Rate)을 곱해 미분 반대 방향으로 이동\n",
        "    learning_rate = 0.01\n",
        "    w.data -= learning_rate * w.grad\n",
        "    b.data -= learning_rate * b.grad\n",
        "\n",
        "    if i % 5 == 0:\n",
        "        print(f\"Step {i}: Loss = {loss.data:.4f}, Pred = {pred.data:.4f}\")\n",
        "\n",
        "print(f\"\\n최종 결과 -> w: {w.data:.2f}, b: {b.data:.2f}\")\n",
        "print(f\"최종 예측값: {w.data * x + b.data:.2f} (정답 5.0에 근접!)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjXkOzRBzAyu",
        "outputId": "d022132e-b2b6-4744-dd83-366513be57c2"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0: Loss = 16.0000, Pred = 1.0000\n",
            "Step 5: Loss = 5.5789, Pred = 2.6380\n",
            "Step 10: Loss = 1.9452, Pred = 3.6053\n",
            "Step 15: Loss = 0.6783, Pred = 4.1764\n",
            "Step 20: Loss = 0.2365, Pred = 4.5137\n",
            "Step 25: Loss = 0.0825, Pred = 4.7128\n",
            "Step 30: Loss = 0.0288, Pred = 4.8304\n",
            "Step 35: Loss = 0.0100, Pred = 4.8999\n",
            "Step 40: Loss = 0.0035, Pred = 4.9409\n",
            "Step 45: Loss = 0.0012, Pred = 4.9651\n",
            "\n",
            "최종 결과 -> w: 2.09, b: 0.80\n",
            "최종 예측값: 4.98 (정답 5.0에 근접!)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모듈 4 ▲ GPT-2 유사 신경망 구조\n",
        "### 4.1. 파라미터 초기화\n"
      ],
      "metadata": {
        "id": "-I0T6rY_zA2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_layer = 1      # 트랜스포머 레이어 수\n",
        "n_embd = 16      # 임베딩 차원\n",
        "block_size = 16  # 최대 컨텍스트 길이\n",
        "n_head = 4       # 멀티헤드 어텐션의 헤드 수\n",
        "head_dim = n_embd // n_head  # 헤드당 차원 = 4"
      ],
      "metadata": {
        "id": "r_rpmD0c6IrM"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2. 전체 신경망 흐름 (gpt 함수)\n",
        "- Transformer Language Model Architecture\n",
        "\n",
        " 1️⃣ Input Stage\n",
        "\n",
        "$$\n",
        "\\text{Token ID} + \\text{Position ID}\n",
        "$$\n",
        "\n",
        "↓\n",
        "\n",
        "Embedding Layer\n",
        "\n",
        "$$\n",
        "\\mathbf{X} = \\text{wte}(\\text{Token}) + \\text{wpe}(\\text{Position})\n",
        "$$\n",
        "\n",
        "- **wte**: Token Embedding  \n",
        "- **wpe**: Position Embedding  \n",
        "\n",
        "↓\n",
        "\n",
        "RMS Normalization\n",
        "\n",
        "$$\n",
        "\\mathbf{X}_{norm} = \\text{RMSNorm}(\\mathbf{X})\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "2️⃣ Transformer Block (Repeated L times)\n",
        "\n",
        "For each layer:\n",
        "\n",
        "(1) RMSNorm\n",
        "\n",
        "$$\n",
        "\\mathbf{H}_1 = \\text{RMSNorm}(\\mathbf{X})\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "(2) Multi-Head Self-Attention\n",
        "\n",
        "Query, Key, Value 계산:\n",
        "\n",
        "$$\n",
        "Q = \\mathbf{H}_1 W_Q\n",
        "$$\n",
        "$$\n",
        "K = \\mathbf{H}_1 W_K\n",
        "$$\n",
        "$$\n",
        "V = \\mathbf{H}_1 W_V\n",
        "$$\n",
        "\n",
        "Scaled Dot-Product Attention:\n",
        "\n",
        "$$\n",
        "\\text{Attention}(Q,K,V) =\n",
        "\\text{Softmax}\\left(\n",
        "\\frac{QK^T}{\\sqrt{d_k}}\n",
        "\\right)V\n",
        "$$\n",
        "\n",
        "Multi-head 결합:\n",
        "\n",
        "$$\n",
        "\\text{MHA} = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h) W_O\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "(3) Residual Connection\n",
        "\n",
        "$$\n",
        "\\mathbf{X}_1 = \\mathbf{X} + \\text{MHA}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "(4) RMSNorm\n",
        "\n",
        "$$\n",
        "\\mathbf{H}_2 = \\text{RMSNorm}(\\mathbf{X}_1)\n",
        "$$\n",
        "\n",
        "---\n",
        "(5) MLP (Feed Forward Network)\n",
        "\n",
        "$$\n",
        "\\mathbf{F} = \\text{FC}_2(\\text{ReLU}(\\text{FC}_1(\\mathbf{H}_2)))\n",
        "$$\n",
        "\n",
        "---\n",
        "(6) Residual Connection\n",
        "\n",
        "$$\n",
        "\\mathbf{X}_{next} = \\mathbf{X}_1 + \\mathbf{F}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "3️⃣ Output Layer\n",
        "\n",
        "Language Modeling Head\n",
        "\n",
        "$$\n",
        "\\text{logits} = \\mathbf{X}_{final} W_{lm}\n",
        "$$\n",
        "\n",
        "↓\n",
        "\n",
        "Next Token Probability\n",
        "\n",
        "$$\n",
        "P(\\text{token}) = \\text{Softmax}(\\text{logits})\n",
        "$$\n"
      ],
      "metadata": {
        "id": "3a8TdKUQ6PYh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "토큰 ID + 위치 ID\n",
        "\n",
        "    ↓\n",
        "[wte] 토큰 임베딩 + [wpe] 위치 임베딩\n",
        "\n",
        "    ↓\n",
        "RMSNorm 정규화\n",
        "    ↓\n",
        "\n",
        "[ 트랜스포머 레이어 반복 ]\n",
        "-  ├─ RMSNorm\n",
        "-  ├─ Multi-Head Self-Attention (Q, K, V 계산 → 소프트맥스 → 가중합)\n",
        "-  ├─ 잔차 연결(Residual)\n",
        "-  ├─ RMSNorm\n",
        "-  ├─ MLP (FC1 → ReLU → FC2)\n",
        "-  └─ 잔차 연결\n",
        "\n",
        "    ↓\n",
        "[lm_head] 선형 변환 → 로짓(logits)"
      ],
      "metadata": {
        "id": "QyQxvJ4Vvhmi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3IyA-eYy63xR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3. 핵심 구성요소 설명\n",
        "**a. 임베딩 (Embedding)**\n",
        "- 토큰 의미 + 위치 정보를 동시에 인코딩합니다.\n"
      ],
      "metadata": {
        "id": "hW3IKVOd7qWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tok_emb = state_dict['wte'][token_id]  # 토큰 → 16차원 벡터\n",
        "pos_emb = state_dict['wpe'][pos_id]    # 위치 → 16차원 벡터\n",
        "x = [t + p for t, p in zip(tok_emb, pos_emb)]  # 합산"
      ],
      "metadata": {
        "id": "N2qbq4EB7zyj"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**b. RMSNorm (Root Mean Square Normalization)**\n",
        "- 학습을 안정화하는 정규화입니다. GPT-2의 LayerNorm을 단순화한 버전입니다.\n"
      ],
      "metadata": {
        "id": "bx2fjYVf7-wH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rmsnorm(x):\n",
        "    ms = sum(xi * xi for xi in x) / len(x)\n",
        "    scale = (ms + 1e-5) ** -0.5\n",
        "    return [xi * scale for xi in x]"
      ],
      "metadata": {
        "id": "MMfyPJ7h7z1O"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**c. Multi-Head Self-Attention**\n"
      ],
      "metadata": {
        "id": "o4yFP9qw8Tfv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q = linear(x, attn_wq)   # Query: \"무엇을 찾고 있나?\"\n",
        "k = linear(x, attn_wk)   # Key:   \"나는 이런 정보다\"\n",
        "v = linear(x, attn_wv)   # Value: \"실제로 전달할 내용\"\n",
        "\n",
        "# 어텐션 스코어 = Q·K / √head_dim\n",
        "# attn_weights = softmax(scores)\n",
        "# output = Σ(attn_weights × V)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "ZlhnKQEU7z4-",
        "outputId": "90469651-0338-49a6-bda8-3575ff7af735"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'attn_wq' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-392613503.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_wq\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# Query: \"무엇을 찾고 있나?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_wk\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# Key:   \"나는 이런 정보다\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_wv\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# Value: \"실제로 전달할 내용\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 어텐션 스코어 = Q·K / √head_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'attn_wq' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1."
      ],
      "metadata": {
        "id": "4hsqFYDD7qmu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-OOYZe_z63z2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OBkG4Bu0632V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oWorhkCF635S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YRCwMcMl638H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The most atomic way to train and run inference for a GPT in pure, dependency-free Python.\n",
        "This file is the complete algorithm.\n",
        "Everything else is just efficiency.\n",
        "\n",
        "@karpathy\n",
        "\"\"\"\n",
        "\n",
        "import os       # os.path.exists\n",
        "import math     # math.log, math.exp\n",
        "import random   # random.seed, random.choices, random.gauss, random.shuffle\n",
        "random.seed(42) # Let there be order among chaos\n",
        "\n",
        "# Let there be a Dataset `docs`: list[str] of documents (e.g. a list of names)\n",
        "if not os.path.exists('input.txt'):\n",
        "    import urllib.request\n",
        "    names_url = 'https://raw.githubusercontent.com/karpathy/makemore/988aa59/names.txt'\n",
        "    urllib.request.urlretrieve(names_url, 'input.txt')\n",
        "docs = [line.strip() for line in open('input.txt') if line.strip()]\n",
        "random.shuffle(docs)\n",
        "docs = docs[:3200]\n",
        "print(f\"num docs: {len(docs)}\")\n",
        "\n",
        "# Let there be a Tokenizer to translate strings to sequences of integers (\"tokens\") and back\n",
        "uchars = sorted(set(''.join(docs))) # unique characters in the dataset become token ids 0..n-1\n",
        "BOS = len(uchars) # token id for a special Beginning of Sequence (BOS) token\n",
        "vocab_size = len(uchars) + 1 # total number of unique tokens, +1 is for BOS\n",
        "print(f\"vocab size: {vocab_size}\")\n",
        "\n",
        "# Let there be Autograd to recursively apply the chain rule through a computation graph\n",
        "class Value:\n",
        "    __slots__ = ('data', 'grad', '_children', '_local_grads') # Python optimization for memory usage\n",
        "\n",
        "    def __init__(self, data, children=(), local_grads=()):\n",
        "        self.data = data                # scalar value of this node calculated during forward pass\n",
        "        self.grad = 0                   # derivative of the loss w.r.t. this node, calculated in backward pass\n",
        "        self._children = children       # children of this node in the computation graph\n",
        "        self._local_grads = local_grads # local derivative of this node w.r.t. its children\n",
        "\n",
        "    def __add__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        return Value(self.data + other.data, (self, other), (1, 1))\n",
        "\n",
        "    def __mul__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        return Value(self.data * other.data, (self, other), (other.data, self.data))\n",
        "\n",
        "    def __pow__(self, other): return Value(self.data**other, (self,), (other * self.data**(other-1),))\n",
        "    def log(self): return Value(math.log(self.data), (self,), (1/self.data,))\n",
        "    def exp(self): return Value(math.exp(self.data), (self,), (math.exp(self.data),))\n",
        "    def relu(self): return Value(max(0, self.data), (self,), (float(self.data > 0),))\n",
        "    def __neg__(self): return self * -1\n",
        "    def __radd__(self, other): return self + other\n",
        "    def __sub__(self, other): return self + (-other)\n",
        "    def __rsub__(self, other): return other + (-self)\n",
        "    def __rmul__(self, other): return self * other\n",
        "    def __truediv__(self, other): return self * other**-1\n",
        "    def __rtruediv__(self, other): return other * self**-1\n",
        "\n",
        "    def backward(self):\n",
        "        topo = []\n",
        "        visited = set()\n",
        "        def build_topo(v):\n",
        "            if v not in visited:\n",
        "                visited.add(v)\n",
        "                for child in v._children:\n",
        "                    build_topo(child)\n",
        "                topo.append(v)\n",
        "        build_topo(self)\n",
        "        self.grad = 1\n",
        "        for v in reversed(topo):\n",
        "            for child, local_grad in zip(v._children, v._local_grads):\n",
        "                child.grad += local_grad * v.grad\n",
        "\n",
        "# Initialize the parameters, to store the knowledge of the model\n",
        "n_layer = 1     # depth of the transformer neural network (number of layers)\n",
        "n_embd = 16     # width of the network (embedding dimension)\n",
        "block_size = 16 # maximum context length of the attention window (note: the longest name is 15 characters)\n",
        "n_head = 4      # number of attention heads\n",
        "head_dim = n_embd // n_head # derived dimension of each head\n",
        "matrix = lambda nout, nin, std=0.08: [[Value(random.gauss(0, std)) for _ in range(nin)] for _ in range(nout)]\n",
        "state_dict = {'wte': matrix(vocab_size, n_embd), 'wpe': matrix(block_size, n_embd), 'lm_head': matrix(vocab_size, n_embd)}\n",
        "for i in range(n_layer):\n",
        "    state_dict[f'layer{i}.attn_wq'] = matrix(n_embd, n_embd)\n",
        "    state_dict[f'layer{i}.attn_wk'] = matrix(n_embd, n_embd)\n",
        "    state_dict[f'layer{i}.attn_wv'] = matrix(n_embd, n_embd)\n",
        "    state_dict[f'layer{i}.attn_wo'] = matrix(n_embd, n_embd)\n",
        "    state_dict[f'layer{i}.mlp_fc1'] = matrix(4 * n_embd, n_embd)\n",
        "    state_dict[f'layer{i}.mlp_fc2'] = matrix(n_embd, 4 * n_embd)\n",
        "params = [p for mat in state_dict.values() for row in mat for p in row] # flatten params into a single list[Value]\n",
        "print(f\"num params: {len(params)}\")\n",
        "\n",
        "# Define the model architecture: a function mapping tokens and parameters to logits over what comes next\n",
        "# Follow GPT-2, blessed among the GPTs, with minor differences: layernorm -> rmsnorm, no biases, GeLU -> ReLU\n",
        "def linear(x, w):\n",
        "    return [sum(wi * xi for wi, xi in zip(wo, x)) for wo in w]\n",
        "\n",
        "def softmax(logits):\n",
        "    max_val = max(val.data for val in logits)\n",
        "    exps = [(val - max_val).exp() for val in logits]\n",
        "    total = sum(exps)\n",
        "    return [e / total for e in exps]\n",
        "\n",
        "def rmsnorm(x):\n",
        "    ms = sum(xi * xi for xi in x) / len(x)\n",
        "    scale = (ms + 1e-5) ** -0.5\n",
        "    return [xi * scale for xi in x]\n",
        "\n",
        "def gpt(token_id, pos_id, keys, values):\n",
        "    tok_emb = state_dict['wte'][token_id] # token embedding\n",
        "    pos_emb = state_dict['wpe'][pos_id] # position embedding\n",
        "    x = [t + p for t, p in zip(tok_emb, pos_emb)] # joint token and position embedding\n",
        "    x = rmsnorm(x) # note: not redundant due to backward pass via the residual connection\n",
        "\n",
        "    for li in range(n_layer):\n",
        "        # 1) Multi-head Attention block\n",
        "        x_residual = x\n",
        "        x = rmsnorm(x)\n",
        "        q = linear(x, state_dict[f'layer{li}.attn_wq'])\n",
        "        k = linear(x, state_dict[f'layer{li}.attn_wk'])\n",
        "        v = linear(x, state_dict[f'layer{li}.attn_wv'])\n",
        "        keys[li].append(k)\n",
        "        values[li].append(v)\n",
        "        x_attn = []\n",
        "        for h in range(n_head):\n",
        "            hs = h * head_dim\n",
        "            q_h = q[hs:hs+head_dim]\n",
        "            k_h = [ki[hs:hs+head_dim] for ki in keys[li]]\n",
        "            v_h = [vi[hs:hs+head_dim] for vi in values[li]]\n",
        "            attn_logits = [sum(q_h[j] * k_h[t][j] for j in range(head_dim)) / head_dim**0.5 for t in range(len(k_h))]\n",
        "            attn_weights = softmax(attn_logits)\n",
        "            head_out = [sum(attn_weights[t] * v_h[t][j] for t in range(len(v_h))) for j in range(head_dim)]\n",
        "            x_attn.extend(head_out)\n",
        "        x = linear(x_attn, state_dict[f'layer{li}.attn_wo'])\n",
        "        x = [a + b for a, b in zip(x, x_residual)]\n",
        "        # 2) MLP block\n",
        "        x_residual = x\n",
        "        x = rmsnorm(x)\n",
        "        x = linear(x, state_dict[f'layer{li}.mlp_fc1'])\n",
        "        x = [xi.relu() for xi in x]\n",
        "        x = linear(x, state_dict[f'layer{li}.mlp_fc2'])\n",
        "        x = [a + b for a, b in zip(x, x_residual)]\n",
        "\n",
        "    logits = linear(x, state_dict['lm_head'])\n",
        "    return logits\n",
        "\n",
        "# Let there be Adam, the blessed optimizer and its buffers\n",
        "learning_rate, beta1, beta2, eps_adam = 0.01, 0.85, 0.99, 1e-8\n",
        "m = [0.0] * len(params) # first moment buffer\n",
        "v = [0.0] * len(params) # second moment buffer\n",
        "\n",
        "# Repeat in sequence\n",
        "num_steps = 1000 # number of training steps\n",
        "for step in range(num_steps):\n",
        "\n",
        "    # Take single document, tokenize it, surround it with BOS special token on both sides\n",
        "    doc = docs[step % len(docs)]\n",
        "    tokens = [BOS] + [uchars.index(ch) for ch in doc] + [BOS]\n",
        "    n = min(block_size, len(tokens) - 1)\n",
        "\n",
        "    # Forward the token sequence through the model, building up the computation graph all the way to the loss\n",
        "    keys, values = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]\n",
        "    losses = []\n",
        "    for pos_id in range(n):\n",
        "        token_id, target_id = tokens[pos_id], tokens[pos_id + 1]\n",
        "        logits = gpt(token_id, pos_id, keys, values)\n",
        "        probs = softmax(logits)\n",
        "        loss_t = -probs[target_id].log()\n",
        "        losses.append(loss_t)\n",
        "    loss = (1 / n) * sum(losses) # final average loss over the document sequence. May yours be low.\n",
        "\n",
        "    # Backward the loss, calculating the gradients with respect to all model parameters\n",
        "    loss.backward()\n",
        "\n",
        "    # Adam optimizer update: update the model parameters based on the corresponding gradients\n",
        "    lr_t = learning_rate * (1 - step / num_steps) # linear learning rate decay\n",
        "    for i, p in enumerate(params):\n",
        "        m[i] = beta1 * m[i] + (1 - beta1) * p.grad\n",
        "        v[i] = beta2 * v[i] + (1 - beta2) * p.grad ** 2\n",
        "        m_hat = m[i] / (1 - beta1 ** (step + 1))\n",
        "        v_hat = v[i] / (1 - beta2 ** (step + 1))\n",
        "        p.data -= lr_t * m_hat / (v_hat ** 0.5 + eps_adam)\n",
        "        p.grad = 0\n",
        "\n",
        "    print(f\"step {step+1:4d} / {num_steps:4d} | loss {loss.data:.4f}\", end='\\r')\n",
        "\n",
        "# Inference: may the model babble back to us\n",
        "temperature = 0.5 # in (0, 1], control the \"creativity\" of generated text, low to high\n",
        "print(\"\\n--- inference (new, hallucinated names) ---\")\n",
        "for sample_idx in range(20):\n",
        "    keys, values = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]\n",
        "    token_id = BOS\n",
        "    sample = []\n",
        "    for pos_id in range(block_size):\n",
        "        logits = gpt(token_id, pos_id, keys, values)\n",
        "        probs = softmax([l / temperature for l in logits])\n",
        "        token_id = random.choices(range(vocab_size), weights=[p.data for p in probs])[0]\n",
        "        if token_id == BOS:\n",
        "            break\n",
        "        sample.append(uchars[token_id])\n",
        "    print(f\"sample {sample_idx+1:2d}: {''.join(sample)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KObH9M-dsTEb",
        "outputId": "d0f7c783-92ee-46ab-8580-614eaaa3637a"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num docs: 3200\n",
            "vocab size: 27\n",
            "num params: 4192\n",
            "step 1000 / 1000 | loss 2.6497\n",
            "--- inference (new, hallucinated names) ---\n",
            "sample  1: kamon\n",
            "sample  2: ann\n",
            "sample  3: karai\n",
            "sample  4: jaire\n",
            "sample  5: vialan\n",
            "sample  6: karia\n",
            "sample  7: yeran\n",
            "sample  8: anna\n",
            "sample  9: areli\n",
            "sample 10: kaina\n",
            "sample 11: konna\n",
            "sample 12: keylen\n",
            "sample 13: liole\n",
            "sample 14: alerin\n",
            "sample 15: earan\n",
            "sample 16: lenne\n",
            "sample 17: kana\n",
            "sample 18: lara\n",
            "sample 19: alela\n",
            "sample 20: anton\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Andrej Karpathy의 minGPT 혹은 micrograd 스타일의 교육용 코드입니다. 직접 구현한 Value 클래스와 수동 연산들을 PyTorch의 핵심 기능인 torch.Tensor와 nn.Module 등을 사용하여 간략하게 리팩토링해 보겠습니다.\n",
        "\n",
        "PyTorch를 사용하면 수동으로 구현했던 Autograd(역전파), Layer 구성, Adam 옵티마이저를 단 몇 줄로 줄일 수 있습니다."
      ],
      "metadata": {
        "id": "QuATcocU-AP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import os, urllib.request, random\n",
        "\n",
        "# 1. 데이터 준비 (기존과 동일)\n",
        "if not os.path.exists('input.txt'):\n",
        "    names_url = 'https://raw.githubusercontent.com/karpathy/makemore/988aa59/names.txt'\n",
        "    urllib.request.urlretrieve(names_url, 'input.txt')\n",
        "docs = [line.strip() for line in open('input.txt') if line.strip()]\n",
        "random.seed(42); random.shuffle(docs)\n",
        "# docs = docs[:3200]\n",
        "\n",
        "uchars = sorted(set(''.join(docs)))\n",
        "vocab_size = len(uchars) + 1\n",
        "BOS = len(uchars)\n",
        "char_to_it = {ch: i for i, ch in enumerate(uchars)}\n",
        "it_to_char = {i: ch for i, ch in enumerate(uchars)}\n",
        "\n",
        "# 2. 하이퍼파라미터\n",
        "n_layer, n_embd, block_size, n_head = 1, 16, 16, 4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# 3. GPT 모델 정의 (PyTorch Module 활용)\n",
        "class SimpleGPT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.wte = nn.Embedding(vocab_size, n_embd)\n",
        "        self.wpe = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[\n",
        "            nn.TransformerEncoderLayer(d_model=n_embd, nhead=n_head, dim_feedforward=4*n_embd,\n",
        "                                       activation='relu', batch_first=True, norm_first=True)\n",
        "            for _ in range(n_layer)\n",
        "        ])\n",
        "        self.ln_f = nn.RMSNorm(n_embd) # PyTorch 최신 버전 기준\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, idx):\n",
        "        b, t = idx.size()\n",
        "        pos = torch.arange(0, t, device=device).unsqueeze(0)\n",
        "        x = self.wte(idx) + self.wpe(pos)\n",
        "\n",
        "        # 인과적 마스킹(Causal Mask) 적용\n",
        "        mask = torch.triu(torch.ones(t, t, device=device) * float('-inf'), diagonal=1)\n",
        "        x = self.blocks[0].self_attn(x, x, x, attn_mask=mask)[0] + x # 단순화를 위해 레이어 직접 호출\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "        return logits\n",
        "\n",
        "model = SimpleGPT().to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01, betas=(0.85, 0.99))\n",
        "\n",
        "# 4. 학습 루프\n",
        "for step in range(1000):\n",
        "    doc = docs[step % len(docs)]\n",
        "    tokens = [BOS] + [char_to_it[ch] for ch in doc] + [BOS]\n",
        "    x_idx = torch.tensor([tokens[:-1]], device=device)\n",
        "    y_idx = torch.tensor([tokens[1:]], device=device)\n",
        "\n",
        "    logits = model(x_idx)\n",
        "    loss = F.cross_entropy(logits.view(-1, vocab_size), y_idx.view(-1))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if step % 100 == 0:\n",
        "        print(f\"step {step} | loss {loss.item():.4f}\")\n",
        "\n",
        "# 5. 추론 (Inference)\n",
        "print(\"\\n--- 생성된 이름 ---\")\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for _ in range(10):\n",
        "        out = []\n",
        "        idx = torch.tensor([[BOS]], device=device)\n",
        "        for _ in range(block_size):\n",
        "            logits = model(idx)[:, -1, :] / 0.5\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_id = torch.multinomial(probs, num_samples=1)\n",
        "            if next_id.item() == BOS: break\n",
        "            out.append(it_to_char[next_id.item()])\n",
        "            idx = torch.cat((idx, next_id), dim=1)\n",
        "        print(\"\".join(out))"
      ],
      "metadata": {
        "id": "arsgkcFPzhqn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f340e4e-fc7a-479a-8ed0-5850159c7a1b"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0 | loss 3.5359\n",
            "step 100 | loss 2.5243\n",
            "step 200 | loss 2.2194\n",
            "step 300 | loss 2.6814\n",
            "step 400 | loss 2.5972\n",
            "step 500 | loss 2.4916\n",
            "step 600 | loss 2.2032\n",
            "step 700 | loss 2.3732\n",
            "step 800 | loss 2.2603\n",
            "step 900 | loss 2.0710\n",
            "\n",
            "--- 생성된 이름 ---\n",
            "jerel\n",
            "nariy\n",
            "canera\n",
            "solalin\n",
            "anon\n",
            "monne\n",
            "lii\n",
            "shona\n",
            "nerela\n",
            "lleen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2-YtXXfD-DEA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}